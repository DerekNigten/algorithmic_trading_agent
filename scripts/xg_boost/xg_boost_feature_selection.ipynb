{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70f3c0d-cebc-4ec7-8dbc-9165ce0ffdf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     13\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_processed_data\u001b[39m():\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load processed features from disk.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature selection module\n",
    "Uses Mutual Information and Spearman correlation filtering\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from config import *\n",
    "\n",
    "\n",
    "def load_processed_data():\n",
    "    \"\"\"Load processed features from disk.\"\"\"\n",
    "    data_path = OUTPUT_DIR / OUTPUT_FILES['processed_data']\n",
    "    print(f\"Loading processed data from: {data_path}\")\n",
    "    features = joblib.load(data_path)\n",
    "    print(f\"Loaded {features.shape[0]:,} samples with {features.shape[1]} columns\")\n",
    "    return features\n",
    "\n",
    "\n",
    "def select_features(df, events_ahead=None, max_corr=None):\n",
    "    \"\"\"\n",
    "    Select features using MI scores and Spearman correlation filtering.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with features and target\n",
    "        events_ahead: Events ahead for MI calculation (uses MI_EVENTS_AHEAD if None)\n",
    "        max_corr: Max correlation threshold (uses MAX_CORRELATION if None)\n",
    "    \n",
    "    Returns:\n",
    "        List of selected feature names\n",
    "    \"\"\"\n",
    "    if events_ahead is None:\n",
    "        events_ahead = MI_EVENTS_AHEAD\n",
    "    if max_corr is None:\n",
    "        max_corr = MAX_CORRELATION\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE SELECTION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Events ahead for MI calculation: {events_ahead}\")\n",
    "    print(f\"Max correlation threshold: {max_corr}\")\n",
    "    \n",
    "    # Create temporary label for feature selection\n",
    "    temp_df = df.copy()\n",
    "    temp_df[\"label\"] = 0\n",
    "    temp_df.loc[\n",
    "        temp_df[\"mid_price\"].shift(-events_ahead) > temp_df[\"mid_price\"], \n",
    "        \"label\"\n",
    "    ] = 1\n",
    "    temp_df.loc[\n",
    "        temp_df[\"mid_price\"].shift(-events_ahead) < temp_df[\"mid_price\"], \n",
    "        \"label\"\n",
    "    ] = -1\n",
    "\n",
    "    feature_names = [\n",
    "        col for col in temp_df.columns \n",
    "        if col not in ['target', 'date', 'label']\n",
    "    ]\n",
    "    temp_df = temp_df.dropna()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # COMPUTE MUTUAL INFORMATION SCORES\n",
    "    # ========================================================================\n",
    "    print(\"\\nComputing Mutual Information scores...\")\n",
    "    X = temp_df[feature_names].values\n",
    "    y = temp_df['label'].values\n",
    "    \n",
    "    mi_scores = mutual_info_classif(X, y, random_state=RANDOM_STATE, n_neighbors=3)\n",
    "    mi_dict = dict(zip(feature_names, mi_scores))\n",
    "    \n",
    "    selected_features = feature_names.copy()\n",
    "    print(f\"Initial features: {len(selected_features)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ITERATIVE CORRELATION-BASED REMOVAL\n",
    "    # ========================================================================\n",
    "    print(\"\\nIterative correlation-based removal:\")\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        \n",
    "        # Compute correlation matrix\n",
    "        corr_matrix, _ = spearmanr(temp_df[selected_features].values)\n",
    "        \n",
    "        max_correlation = -1\n",
    "        remove_feat = None\n",
    "        keep_feat = None\n",
    "        \n",
    "        # Find highest correlation pair\n",
    "        for i in range(len(selected_features)):\n",
    "            for j in range(i+1, len(selected_features)):\n",
    "                corr_val = abs(corr_matrix[i, j])\n",
    "                \n",
    "                if corr_val > max_corr and corr_val > max_correlation:\n",
    "                    max_correlation = corr_val\n",
    "                    feat1 = selected_features[i]\n",
    "                    feat2 = selected_features[j]\n",
    "                    \n",
    "                    # Remove feature with lower MI score\n",
    "                    if mi_dict[feat1] < mi_dict[feat2]:\n",
    "                        remove_feat = feat1\n",
    "                        keep_feat = feat2\n",
    "                    else:\n",
    "                        remove_feat = feat2\n",
    "                        keep_feat = feat1\n",
    "        \n",
    "        # Stop if no high correlations found\n",
    "        if remove_feat is None:\n",
    "            break\n",
    "        \n",
    "        print(f\"  Iter {iteration}: Removing '{remove_feat}' \"\n",
    "              f\"(corr={max_correlation:.3f} with '{keep_feat}', \"\n",
    "              f\"MI={mi_dict[remove_feat]:.4f})\")\n",
    "        \n",
    "        selected_features.remove(remove_feat)\n",
    "    \n",
    "    print(f\"\\nFinal features: {len(selected_features)}\")\n",
    "    print(f\"Removed: {len(feature_names) - len(selected_features)} features\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # DISPLAY SELECTED FEATURES WITH MI SCORES\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SELECTED FEATURES WITH MI SCORES (sorted by MI)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    selected_mi = [(feat, mi_dict[feat]) for feat in selected_features]\n",
    "    selected_mi.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for feat, mi_score in selected_mi:\n",
    "        print(f\"{feat:30s} {mi_score:.6f}\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "\n",
    "def normalize_and_split(features, selected_features, \n",
    "                        train_dates=None, val_dates=None, test_dates=None):\n",
    "    \"\"\"\n",
    "    Split by date and normalize features.\n",
    "    \n",
    "    Args:\n",
    "        features: DataFrame with all features\n",
    "        selected_features: List of feature names to use\n",
    "        train_dates: List of training dates (uses TRAIN_DATES if None)\n",
    "        val_dates: List of validation dates (uses VAL_DATES if None)\n",
    "        test_dates: List of test dates (uses TEST_DATES if None)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_train, y_train, X_val, y_val, X_test, y_test, scaler)\n",
    "    \"\"\"\n",
    "    if train_dates is None:\n",
    "        train_dates = TRAIN_DATES\n",
    "    if val_dates is None:\n",
    "        val_dates = VAL_DATES\n",
    "    if test_dates is None:\n",
    "        test_dates = TEST_DATES\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA SPLITTING AND NORMALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Split by date\n",
    "    train_df = features[features['date'].isin(train_dates)].copy()\n",
    "    val_df = features[features['date'].isin(val_dates)].copy()\n",
    "    test_df = features[features['date'].isin(test_dates)].copy()\n",
    "    \n",
    "    print(f\"Train: {len(train_df):,} events (days {', '.join(train_dates)})\")\n",
    "    print(f\"Val:   {len(val_df):,} events (days {', '.join(val_dates)})\")\n",
    "    print(f\"Test:  {len(test_df):,} events (days {', '.join(test_dates)})\")\n",
    "    \n",
    "    # Keep only selected features and target\n",
    "    train_df = train_df[selected_features + ['target']]\n",
    "    val_df = val_df[selected_features + ['target']]\n",
    "    test_df = test_df[selected_features + ['target']]\n",
    "    \n",
    "    # Normalize using StandardScaler\n",
    "    print(\"\\nNormalizing features using StandardScaler...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df[selected_features])\n",
    "    \n",
    "    train_df[selected_features] = scaler.transform(train_df[selected_features])\n",
    "    val_df[selected_features] = scaler.transform(val_df[selected_features])\n",
    "    test_df[selected_features] = scaler.transform(test_df[selected_features])\n",
    "    \n",
    "    # Print label distribution\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "        counts = df['target'].value_counts().sort_index()\n",
    "        total = len(df)\n",
    "        print(f\"\\n{name} set ({total:,} samples):\")\n",
    "        print(f\"  Down (0):    {counts.get(0, 0):6,} \"\n",
    "              f\"({counts.get(0, 0)/total*100:5.2f}%)\")\n",
    "        print(f\"  Neutral (1): {counts.get(1, 0):6,} \"\n",
    "              f\"({counts.get(1, 0)/total*100:5.2f}%)\")\n",
    "        print(f\"  Up (2):      {counts.get(2, 0):6,} \"\n",
    "              f\"({counts.get(2, 0)/total*100:5.2f}%)\")\n",
    "    \n",
    "    # Split into X and y\n",
    "    X_train, y_train = train_df[selected_features], train_df['target']\n",
    "    X_val, y_val = val_df[selected_features], val_df['target']\n",
    "    X_test, y_test = test_df[selected_features], test_df['target']\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, scaler\n",
    "\n",
    "\n",
    "def save_selection_outputs(selected_features, scaler):\n",
    "    \"\"\"Save selected features and scaler to disk.\"\"\"\n",
    "    # Save selected features\n",
    "    features_path = OUTPUT_DIR / OUTPUT_FILES['selected_features']\n",
    "    joblib.dump(selected_features, features_path)\n",
    "    print(f\"\\n✓ Saved selected features to: {features_path}\")\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = OUTPUT_DIR / OUTPUT_FILES['scaler']\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"✓ Saved scaler to: {scaler_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main feature selection pipeline.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"FEATURE SELECTION PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load processed data\n",
    "    features = load_processed_data()\n",
    "    \n",
    "    # Select features using training data only\n",
    "    train_features = features[features['date'].isin(TRAIN_DATES)].copy()\n",
    "    selected_features = select_features(train_features)\n",
    "    \n",
    "    # Normalize and split\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, scaler = normalize_and_split(\n",
    "        features, selected_features\n",
    "    )\n",
    "    \n",
    "    # Save outputs\n",
    "    save_selection_outputs(selected_features, scaler)\n",
    "    \n",
    "    # Save split data for training\n",
    "    split_data = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    split_path = OUTPUT_DIR / 'split_data.pkl'\n",
    "    joblib.dump(split_data, split_path)\n",
    "    print(f\"✓ Saved split data to: {split_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE SELECTION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Selected features: {len(selected_features)}\")\n",
    "    print(f\"Train samples: {len(X_train):,}\")\n",
    "    print(f\"Val samples: {len(X_val):,}\")\n",
    "    print(f\"Test samples: {len(X_test):,}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
