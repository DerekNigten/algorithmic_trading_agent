{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b2ff7-c0b9-4293-9dee-b212b71006e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameter tuning module\n",
    "Two-stage grid search for optimal XGBoost parameters\n",
    "\"\"\"\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from config import *\n",
    "\n",
    "\n",
    "def load_split_data():\n",
    "    \"\"\"Load train/val/test splits from disk.\"\"\"\n",
    "    split_path = OUTPUT_DIR / 'split_data.pkl'\n",
    "    print(f\"Loading split data from: {split_path}\")\n",
    "    split_data = joblib.load(split_path)\n",
    "    \n",
    "    X_train = split_data['X_train']\n",
    "    y_train = split_data['y_train']\n",
    "    X_val = split_data['X_val']\n",
    "    y_val = split_data['y_val']\n",
    "    X_test = split_data['X_test']\n",
    "    y_test = split_data['y_test']\n",
    "    \n",
    "    print(f\"Train: {len(X_train):,} samples\")\n",
    "    print(f\"Val:   {len(X_val):,} samples\")\n",
    "    print(f\"Test:  {len(X_test):,} samples\")\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def tune_stage1(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Stage 1: Coarse grid search.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (best_params, history)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STAGE 1: COARSE GRID SEARCH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_params = None\n",
    "    history = []\n",
    "    \n",
    "    # Calculate total combinations\n",
    "    total = (len(STAGE1_GRID['max_depth']) * \n",
    "             len(STAGE1_GRID['learning_rate']) * \n",
    "             len(STAGE1_GRID['n_estimators']) * \n",
    "             len(STAGE1_GRID['subsample']) * \n",
    "             len(STAGE1_GRID['colsample_bytree']) * \n",
    "             len(STAGE1_GRID['min_child_weight']) * \n",
    "             len(STAGE1_GRID['gamma']))\n",
    "    \n",
    "    print(f\"Testing {total} combinations...\\n\")\n",
    "    \n",
    "    current = 0\n",
    "    for max_depth in STAGE1_GRID['max_depth']:\n",
    "        for lr in STAGE1_GRID['learning_rate']:\n",
    "            for n_est in STAGE1_GRID['n_estimators']:\n",
    "                for subsample in STAGE1_GRID['subsample']:\n",
    "                    for colsample in STAGE1_GRID['colsample_bytree']:\n",
    "                        for min_child in STAGE1_GRID['min_child_weight']:\n",
    "                            for gamma in STAGE1_GRID['gamma']:\n",
    "                                current += 1\n",
    "                                \n",
    "                                # Build parameters\n",
    "                                params = {**XGBOOST_BASE_PARAMS}\n",
    "                                params.update({\n",
    "                                    'max_depth': max_depth,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'n_estimators': n_est,\n",
    "                                    'subsample': subsample,\n",
    "                                    'colsample_bytree': colsample,\n",
    "                                    'min_child_weight': min_child,\n",
    "                                    'gamma': gamma\n",
    "                                })\n",
    "                                \n",
    "                                # Train model\n",
    "                                model = xgb.XGBClassifier(**params)\n",
    "                                model.fit(\n",
    "                                    X_train, y_train, \n",
    "                                    eval_set=[(X_val, y_val)],\n",
    "                                    verbose=False\n",
    "                                )\n",
    "                                \n",
    "                                # Evaluate\n",
    "                                y_val_pred = model.predict(X_val)\n",
    "                                val_acc = accuracy_score(y_val, y_val_pred)\n",
    "                                precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                                    y_val, y_val_pred, \n",
    "                                    average='macro', \n",
    "                                    zero_division=0\n",
    "                                )\n",
    "                                \n",
    "                                print(f\"[{current:2d}/{total}] \"\n",
    "                                      f\"d={max_depth}, lr={lr:.2f}, n={n_est:3d}, \"\n",
    "                                      f\"mcw={min_child}, g={gamma:.1f} | \"\n",
    "                                      f\"Acc={val_acc:.4f}, F1={f1:.4f}\")\n",
    "                                \n",
    "                                # Save to history\n",
    "                                history.append({\n",
    "                                    'max_depth': max_depth,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'n_estimators': n_est,\n",
    "                                    'subsample': subsample,\n",
    "                                    'colsample_bytree': colsample,\n",
    "                                    'min_child_weight': min_child,\n",
    "                                    'gamma': gamma,\n",
    "                                    'val_accuracy': float(val_acc),\n",
    "                                    'val_f1': float(f1)\n",
    "                                })\n",
    "                                \n",
    "                                # Update best\n",
    "                                if val_acc > best_acc:\n",
    "                                    best_acc = val_acc\n",
    "                                    best_params = params.copy()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STAGE 1 BEST: Acc={best_acc:.4f}\")\n",
    "    print(f\"  max_depth: {best_params['max_depth']}\")\n",
    "    print(f\"  learning_rate: {best_params['learning_rate']}\")\n",
    "    print(f\"  n_estimators: {best_params['n_estimators']}\")\n",
    "    print(f\"  min_child_weight: {best_params['min_child_weight']}\")\n",
    "    print(f\"  gamma: {best_params['gamma']}\")\n",
    "    \n",
    "    return best_params, history\n",
    "\n",
    "\n",
    "def tune_stage2(X_train, y_train, X_val, y_val, stage1_best):\n",
    "    \"\"\"\n",
    "    Stage 2: Fine-tune around best parameters from Stage 1.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        stage1_best: Best parameters from Stage 1\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (best_params, history)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STAGE 2: FINE-TUNING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Build refined grid around Stage 1 best\n",
    "    best_depth = stage1_best['max_depth']\n",
    "    best_lr = stage1_best['learning_rate']\n",
    "    best_n = stage1_best['n_estimators']\n",
    "    \n",
    "    stage2_grid = {\n",
    "        'max_depth': [\n",
    "            max(3, best_depth - STAGE2_REFINEMENT['max_depth_range']), \n",
    "            best_depth, \n",
    "            min(10, best_depth + STAGE2_REFINEMENT['max_depth_range'])\n",
    "        ],\n",
    "        'learning_rate': [\n",
    "            max(0.01, best_lr * mult) \n",
    "            for mult in STAGE2_REFINEMENT['learning_rate_multiplier']\n",
    "        ],\n",
    "        'n_estimators': [\n",
    "            max(50, best_n - STAGE2_REFINEMENT['n_estimators_step']), \n",
    "            best_n, \n",
    "            best_n + STAGE2_REFINEMENT['n_estimators_step']\n",
    "        ],\n",
    "        'subsample': STAGE2_REFINEMENT['subsample_options'],\n",
    "        'colsample_bytree': STAGE2_REFINEMENT['colsample_bytree_options'],\n",
    "        'min_child_weight': [stage1_best['min_child_weight']],\n",
    "        'gamma': [stage1_best['gamma']]\n",
    "    }\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_params = None\n",
    "    history = []\n",
    "    \n",
    "    # Calculate total combinations\n",
    "    total = (len(stage2_grid['max_depth']) * \n",
    "             len(stage2_grid['learning_rate']) * \n",
    "             len(stage2_grid['n_estimators']) * \n",
    "             len(stage2_grid['subsample']) * \n",
    "             len(stage2_grid['colsample_bytree']))\n",
    "    \n",
    "    print(f\"Testing {total} refined combinations...\\n\")\n",
    "    \n",
    "    current = 0\n",
    "    for max_depth in stage2_grid['max_depth']:\n",
    "        for lr in stage2_grid['learning_rate']:\n",
    "            for n_est in stage2_grid['n_estimators']:\n",
    "                for subsample in stage2_grid['subsample']:\n",
    "                    for colsample in stage2_grid['colsample_bytree']:\n",
    "                        current += 1\n",
    "                        \n",
    "                        # Build parameters\n",
    "                        params = {**XGBOOST_BASE_PARAMS}\n",
    "                        params.update({\n",
    "                            'max_depth': max_depth,\n",
    "                            'learning_rate': lr,\n",
    "                            'n_estimators': n_est,\n",
    "                            'subsample': subsample,\n",
    "                            'colsample_bytree': colsample,\n",
    "                            'min_child_weight': stage2_grid['min_child_weight'][0],\n",
    "                            'gamma': stage2_grid['gamma'][0]\n",
    "                        })\n",
    "                        \n",
    "                        # Train model\n",
    "                        model = xgb.XGBClassifier(**params)\n",
    "                        model.fit(\n",
    "                            X_train, y_train,\n",
    "                            eval_set=[(X_val, y_val)],\n",
    "                            verbose=False\n",
    "                        )\n",
    "                        \n",
    "                        # Evaluate\n",
    "                        y_val_pred = model.predict(X_val)\n",
    "                        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "                        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                            y_val, y_val_pred, \n",
    "                            average='macro', \n",
    "                            zero_division=0\n",
    "                        )\n",
    "                        \n",
    "                        if current % 10 == 0 or val_acc > best_acc:\n",
    "                            print(f\"[{current:2d}/{total}] \"\n",
    "                                  f\"d={max_depth}, lr={lr:.3f}, n={n_est:3d}, \"\n",
    "                                  f\"ss={subsample:.2f}, cs={colsample:.2f} | \"\n",
    "                                  f\"Acc={val_acc:.4f}, F1={f1:.4f}\")\n",
    "                        \n",
    "                        # Save to history\n",
    "                        history.append({\n",
    "                            'max_depth': max_depth,\n",
    "                            'learning_rate': lr,\n",
    "                            'n_estimators': n_est,\n",
    "                            'subsample': subsample,\n",
    "                            'colsample_bytree': colsample,\n",
    "                            'val_accuracy': float(val_acc),\n",
    "                            'val_f1': float(f1)\n",
    "                        })\n",
    "                        \n",
    "                        # Update best\n",
    "                        if val_acc > best_acc:\n",
    "                            best_acc = val_acc\n",
    "                            best_params = params\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL BEST PARAMETERS (Val Accuracy: {best_acc:.4f})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  max_depth: {best_params['max_depth']}\")\n",
    "    print(f\"  learning_rate: {best_params['learning_rate']:.4f}\")\n",
    "    print(f\"  n_estimators: {best_params['n_estimators']}\")\n",
    "    print(f\"  subsample: {best_params['subsample']:.2f}\")\n",
    "    print(f\"  colsample_bytree: {best_params['colsample_bytree']:.2f}\")\n",
    "    print(f\"  min_child_weight: {best_params['min_child_weight']}\")\n",
    "    print(f\"  gamma: {best_params['gamma']}\")\n",
    "    \n",
    "    return best_params, history\n",
    "\n",
    "\n",
    "def save_tuning_results(stage1_best, stage2_best, stage1_history, stage2_history):\n",
    "    \"\"\"Save tuning history to disk.\"\"\"\n",
    "    full_history = {\n",
    "        'stage1': stage1_history,\n",
    "        'stage2': stage2_history,\n",
    "        'stage1_best': stage1_best,\n",
    "        'stage2_best': stage2_best\n",
    "    }\n",
    "    \n",
    "    history_path = OUTPUT_DIR / 'tuning_history.json'\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(full_history, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Saved tuning history to: {history_path}\")\n",
    "    \n",
    "    # Also save just the best parameters separately\n",
    "    best_params_path = OUTPUT_DIR / 'best_params.json'\n",
    "    with open(best_params_path, 'w') as f:\n",
    "        json.dump(stage2_best, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved best parameters to: {best_params_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main hyperparameter tuning pipeline.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"HYPERPARAMETER TUNING PIPELINE\")\n",
    "    print(\"Two-Stage Grid Search\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_split_data()\n",
    "    \n",
    "    # Stage 1: Coarse search\n",
    "    stage1_best, stage1_history = tune_stage1(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Stage 2: Fine-tuning\n",
    "    stage2_best, stage2_history = tune_stage2(\n",
    "        X_train, y_train, X_val, y_val, stage1_best\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    save_tuning_results(stage1_best, stage2_best, stage1_history, stage2_history)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Stage 1: Tested {len(stage1_history)} combinations\")\n",
    "    print(f\"Stage 2: Tested {len(stage2_history)} combinations\")\n",
    "    print(f\"Total combinations: {len(stage1_history) + len(stage2_history)}\")\n",
    "    print(f\"\\nBest parameters saved to: {OUTPUT_DIR / 'best_params.json'}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
