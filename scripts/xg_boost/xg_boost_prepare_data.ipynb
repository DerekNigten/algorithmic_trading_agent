{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec07326-4cfc-406c-8eb9-ffc0c5f40fb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     10\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_orderbook_data\u001b[39m(dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Load IEX order book snapshots from multiple dates.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m        DataFrame with combined order book data\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data preparation and feature engineering module\n",
    "Loads raw order book data and creates features\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from config import *\n",
    "\n",
    "\n",
    "def fetch_orderbook_data(dates=None):\n",
    "    \"\"\"\n",
    "    Load IEX order book snapshots from multiple dates.\n",
    "    \n",
    "    Args:\n",
    "        dates: List of date strings (format: 'YYYYMMDD'). \n",
    "               If None, uses ALL_DATES from config.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with combined order book data\n",
    "    \"\"\"\n",
    "    if dates is None:\n",
    "        dates = ALL_DATES\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"LOADING RAW ORDER BOOK DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    dfs = []\n",
    "    for date in dates:\n",
    "        file_path = RAW_DATA_PATH / f'{date}_book_updates.csv.gz'\n",
    "        print(f\"  Loading {date}...\")\n",
    "        \n",
    "        df = pd.read_csv(file_path, compression='gzip')\n",
    "        df['date'] = date\n",
    "        df['COLLECTION_TIME'] = pd.to_datetime(df['COLLECTION_TIME'])\n",
    "        df = df.set_index('COLLECTION_TIME')\n",
    "        df = df.between_time(START_TIME, END_TIME)\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Total events loaded: {len(combined):,}\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "def add_all_features(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive order book features.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw order book DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CREATING FEATURES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    features['date'] = df['date']\n",
    "    \n",
    "    # ========================================================================\n",
    "    # BASIC LEVEL-1 FEATURES\n",
    "    # ========================================================================\n",
    "    features[\"mid_price\"] = (df[\"BID_PRICE_1\"] + df[\"ASK_PRICE_1\"]) / 2\n",
    "    features[\"microprice\"] = (\n",
    "        df[\"BID_PRICE_1\"] * df[\"ASK_SIZE_1\"] + \n",
    "        df[\"ASK_PRICE_1\"] * df[\"BID_SIZE_1\"]\n",
    "    ) / (df[\"BID_SIZE_1\"] + df[\"ASK_SIZE_1\"] + 1e-10)\n",
    "    \n",
    "    features[\"spread\"] = df[\"ASK_PRICE_1\"] - df[\"BID_PRICE_1\"]\n",
    "    features[\"vol_imbalance\"] = (\n",
    "        (df[\"BID_SIZE_1\"] - df[\"ASK_SIZE_1\"]) / \n",
    "        (df[\"BID_SIZE_1\"] + df[\"ASK_SIZE_1\"] + 1e-6)\n",
    "    )\n",
    "    features[\"bid_ask_spread_ratio\"] = features[\"spread\"] / features[\"mid_price\"]\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ALL LEVEL PRICES AND SIZES\n",
    "    # ========================================================================\n",
    "    for level in range(1, 4):\n",
    "        features[f\"BID_PRICE_{level}\"] = df[f\"BID_PRICE_{level}\"]\n",
    "        features[f\"BID_SIZE_{level}\"] = df[f\"BID_SIZE_{level}\"]\n",
    "        features[f\"ASK_PRICE_{level}\"] = df[f\"ASK_PRICE_{level}\"]\n",
    "        features[f\"ASK_SIZE_{level}\"] = df[f\"ASK_SIZE_{level}\"]\n",
    "    \n",
    "    # ========================================================================\n",
    "    # AGGREGATE STATISTICS ACROSS LEVELS\n",
    "    # ========================================================================\n",
    "    features[\"bid_price_mean\"] = (\n",
    "        df[\"BID_PRICE_1\"] + df[\"BID_PRICE_2\"] + df[\"BID_PRICE_3\"]\n",
    "    ) / 3\n",
    "    features[\"ask_price_mean\"] = (\n",
    "        df[\"ASK_PRICE_1\"] + df[\"ASK_PRICE_2\"] + df[\"ASK_PRICE_3\"]\n",
    "    ) / 3\n",
    "    features[\"bid_qty_mean\"] = (\n",
    "        df[\"BID_SIZE_1\"] + df[\"BID_SIZE_2\"] + df[\"BID_SIZE_3\"]\n",
    "    ) / 3\n",
    "    features[\"ask_qty_mean\"] = (\n",
    "        df[\"ASK_SIZE_1\"] + df[\"ASK_SIZE_2\"] + df[\"ASK_SIZE_3\"]\n",
    "    ) / 3\n",
    "    \n",
    "    # Cumulative differences\n",
    "    features[\"price_cum_diff\"] = (\n",
    "        (df[\"ASK_PRICE_1\"] - df[\"BID_PRICE_1\"]) + \n",
    "        (df[\"ASK_PRICE_2\"] - df[\"BID_PRICE_2\"]) + \n",
    "        (df[\"ASK_PRICE_3\"] - df[\"BID_PRICE_3\"])\n",
    "    )\n",
    "    features[\"qty_cum_diff\"] = (\n",
    "        (df[\"ASK_SIZE_1\"] - df[\"BID_SIZE_1\"]) + \n",
    "        (df[\"ASK_SIZE_2\"] - df[\"BID_SIZE_2\"]) + \n",
    "        (df[\"ASK_SIZE_3\"] - df[\"BID_SIZE_3\"])\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PRICE MOMENTUM\n",
    "    # ========================================================================\n",
    "    features[\"mid_diff\"] = features[\"mid_price\"].diff()\n",
    "    features[\"mid_return\"] = features[\"mid_diff\"] / features[\"mid_price\"].shift(1)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ORDER FLOW IMBALANCE (OFI)\n",
    "    # ========================================================================\n",
    "    total_bid_qty = df[\"BID_SIZE_1\"] + df[\"BID_SIZE_2\"] + df[\"BID_SIZE_3\"]\n",
    "    total_ask_qty = df[\"ASK_SIZE_1\"] + df[\"ASK_SIZE_2\"] + df[\"ASK_SIZE_3\"]\n",
    "    \n",
    "    bid_qty_change = total_bid_qty.diff()\n",
    "    ask_qty_change = total_ask_qty.diff()\n",
    "    features[\"OFI\"] = bid_qty_change - ask_qty_change\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MOVING AVERAGES\n",
    "    # ========================================================================\n",
    "    features[\"mv_1s\"] = features[\"mid_price\"].rolling(\n",
    "        MA_WINDOW_1S, min_periods=1\n",
    "    ).mean()\n",
    "    features[\"mv_5s\"] = features[\"mid_price\"].rolling(\n",
    "        MA_WINDOW_5S, min_periods=1\n",
    "    ).mean()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VOLATILITY\n",
    "    # ========================================================================\n",
    "    for window in VOL_WINDOWS:\n",
    "        features[f\"vol_{window}\"] = features[\"mid_return\"].rolling(\n",
    "            window, min_periods=1\n",
    "        ).std()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RSI (RELATIVE STRENGTH INDEX)\n",
    "    # ========================================================================\n",
    "    delta = features[\"microprice\"].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(RSI_PERIOD, min_periods=1).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(RSI_PERIOD, min_periods=1).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    features[\"rsi_14\"] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXPONENTIAL MOVING AVERAGES\n",
    "    # ========================================================================\n",
    "    features[\"ema_fast\"] = features[\"mid_price\"].ewm(\n",
    "        span=EMA_FAST_SPAN, adjust=False\n",
    "    ).mean()\n",
    "    features[\"ema_slow\"] = features[\"mid_price\"].ewm(\n",
    "        span=EMA_SLOW_SPAN, adjust=False\n",
    "    ).mean()\n",
    "    features[\"ema_diff\"] = features[\"ema_fast\"] - features[\"ema_slow\"]\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLACEHOLDER FEATURES\n",
    "    # ========================================================================\n",
    "    features[\"time_delta\"] = 1  # Placeholder for actual time deltas\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CLEAN UP\n",
    "    # ========================================================================\n",
    "    features = features.ffill().fillna(0)\n",
    "    features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    features = features.ffill().fillna(0)\n",
    "    \n",
    "    print(f\"Created {len(features.columns)-1} features (excluding 'date')\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def add_labels(features, horizon=None):\n",
    "    \"\"\"\n",
    "    Create price movement labels.\n",
    "    \n",
    "    Args:\n",
    "        features: DataFrame with features\n",
    "        horizon: Number of events ahead to predict (uses LABEL_HORIZON if None)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 'target' column added\n",
    "            0 = down, 1 = neutral, 2 = up\n",
    "    \"\"\"\n",
    "    if horizon is None:\n",
    "        horizon = LABEL_HORIZON\n",
    "    \n",
    "    print(f\"\\nCreating labels (horizon = {horizon} events ahead)...\")\n",
    "    \n",
    "    features['future_price'] = features['microprice'].shift(-horizon)\n",
    "    price_change = features['future_price'] - features['microprice']\n",
    "    \n",
    "    features['target'] = 1  # neutral (no change)\n",
    "    features.loc[price_change > 0, 'target'] = 2  # up\n",
    "    features.loc[price_change < 0, 'target'] = 0  # down\n",
    "    \n",
    "    features.drop('future_price', axis=1, inplace=True)\n",
    "    \n",
    "    # Print label distribution\n",
    "    counts = features['target'].value_counts().sort_index()\n",
    "    total = len(features)\n",
    "    print(f\"Label distribution:\")\n",
    "    print(f\"  Down (0):    {counts.get(0, 0):,} ({counts.get(0, 0)/total*100:.2f}%)\")\n",
    "    print(f\"  Neutral (1): {counts.get(1, 0):,} ({counts.get(1, 0)/total*100:.2f}%)\")\n",
    "    print(f\"  Up (2):      {counts.get(2, 0):,} ({counts.get(2, 0)/total*100:.2f}%)\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def save_processed_data(features):\n",
    "    \"\"\"Save processed features to disk.\"\"\"\n",
    "    output_path = OUTPUT_DIR / OUTPUT_FILES['processed_data']\n",
    "    joblib.dump(features, output_path)\n",
    "    print(f\"\\nâœ“ Saved processed data to: {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main data preparation pipeline.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DATA PREPARATION PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load raw data\n",
    "    df = fetch_orderbook_data()\n",
    "    \n",
    "    # Create features\n",
    "    features = add_all_features(df)\n",
    "    \n",
    "    # Add labels\n",
    "    features = add_labels(features)\n",
    "    \n",
    "    # Save processed data\n",
    "    save_processed_data(features)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA PREPARATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Output: {OUTPUT_DIR / OUTPUT_FILES['processed_data']}\")\n",
    "    print(f\"Shape: {features.shape}\")\n",
    "    print(f\"Features: {len([c for c in features.columns if c not in ['date', 'target']])}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
