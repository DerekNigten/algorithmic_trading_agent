{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a9fcc-4c65-4563-8c7e-cce144c3f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final model training and evaluation module\n",
    "Trains the XGBoost model with best parameters and generates predictions\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support,\n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from config import *\n",
    "\n",
    "\n",
    "def load_split_data():\n",
    "    \"\"\"Load train/val/test splits from disk.\"\"\"\n",
    "    split_path = OUTPUT_DIR / 'split_data.pkl'\n",
    "    print(f\"Loading split data from: {split_path}\")\n",
    "    split_data = joblib.load(split_path)\n",
    "    \n",
    "    X_train = split_data['X_train']\n",
    "    y_train = split_data['y_train']\n",
    "    X_val = split_data['X_val']\n",
    "    y_val = split_data['y_val']\n",
    "    X_test = split_data['X_test']\n",
    "    y_test = split_data['y_test']\n",
    "    \n",
    "    print(f\"Train: {len(X_train):,} samples\")\n",
    "    print(f\"Val:   {len(X_val):,} samples\")\n",
    "    print(f\"Test:  {len(X_test):,} samples\")\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def load_best_params():\n",
    "    \"\"\"Load best hyperparameters from tuning.\"\"\"\n",
    "    params_path = OUTPUT_DIR / 'best_params.json'\n",
    "    print(f\"\\nLoading best parameters from: {params_path}\")\n",
    "    \n",
    "    with open(params_path, 'r') as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    print(\"Best parameters:\")\n",
    "    for key, value in params.items():\n",
    "        if key not in ['objective', 'num_class', 'eval_metric', 'early_stopping_rounds']:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def train_final_model(X_train, y_train, X_val, y_val, X_test, y_test, params):\n",
    "    \"\"\"\n",
    "    Train final XGBoost model with best parameters.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        X_test, y_test: Test data\n",
    "        params: Best hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (model, results, y_train_proba, y_val_proba, y_test_proba)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING FINAL MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Model training complete\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    y_train_proba = model.predict_proba(X_train)\n",
    "    y_val_proba = model.predict_proba(X_val)\n",
    "    y_test_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {}\n",
    "    \n",
    "    for name, y_true, y_pred in [\n",
    "        ('train', y_train, y_train_pred),\n",
    "        ('val', y_val, y_val_pred),\n",
    "        ('test', y_test, y_test_pred)\n",
    "    ]:\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average='macro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': float(acc),\n",
    "            'precision': float(precision),\n",
    "            'recall': float(recall),\n",
    "            'f1': float(f1)\n",
    "        }\n",
    "    \n",
    "    # Print performance summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL PERFORMANCE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Set':<10} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "    print(\"-\"*60)\n",
    "    for name in ['train', 'val', 'test']:\n",
    "        r = results[name]\n",
    "        print(f\"{name.capitalize():<10} {r['accuracy']:.4f}       \"\n",
    "              f\"{r['precision']:.4f}       {r['recall']:.4f}       {r['f1']:.4f}\")\n",
    "    \n",
    "    # Detailed test set report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED TEST SET CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(\n",
    "        y_test, y_test_pred, \n",
    "        target_names=['Down (0)', 'Neutral (1)', 'Up (2)']\n",
    "    ))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"Confusion Matrix (Test Set):\")\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(cm)\n",
    "    print(\"\\nRows = Actual, Columns = Predicted\")\n",
    "    print(\"Order: [Down, Neutral, Up]\")\n",
    "    \n",
    "    return model, results, y_train_proba, y_val_proba, y_test_proba\n",
    "\n",
    "\n",
    "def save_model_and_predictions(model, results, \n",
    "                               X_train, X_val, X_test,\n",
    "                               y_train, y_val, y_test,\n",
    "                               y_train_proba, y_val_proba, y_test_proba,\n",
    "                               selected_features):\n",
    "    \"\"\"\n",
    "    Save trained model, predictions, and analysis.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained XGBoost model\n",
    "        results: Performance metrics dict\n",
    "        X_train, X_val, X_test: Feature matrices\n",
    "        y_train, y_val, y_test: True labels\n",
    "        y_train_proba, y_val_proba, y_test_proba: Prediction probabilities\n",
    "        selected_features: List of feature names\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAVING OUTPUTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Save model\n",
    "    # ========================================================================\n",
    "    model_path = OUTPUT_DIR / OUTPUT_FILES['model']\n",
    "    model.save_model(str(model_path))\n",
    "    print(f\"\\n✓ {OUTPUT_FILES['model']}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Save training history\n",
    "    # ========================================================================\n",
    "    # Load tuning history\n",
    "    tuning_path = OUTPUT_DIR / 'tuning_history.json'\n",
    "    with open(tuning_path, 'r') as f:\n",
    "        tuning_history = json.load(f)\n",
    "    \n",
    "    history = {\n",
    "        'hyperparameter_tuning': tuning_history,\n",
    "        'final_results': results,\n",
    "        'num_features': len(selected_features),\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    history_path = OUTPUT_DIR / OUTPUT_FILES['training_history']\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"✓ {OUTPUT_FILES['training_history']}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Save predictions\n",
    "    # ========================================================================\n",
    "    # Training predictions\n",
    "    train_predictions = pd.DataFrame({\n",
    "        'actual': y_train.values,\n",
    "        'predicted': y_train_proba.argmax(axis=1),\n",
    "        'prob_down': y_train_proba[:, 0],\n",
    "        'prob_neutral': y_train_proba[:, 1],\n",
    "        'prob_up': y_train_proba[:, 2]\n",
    "    })\n",
    "    train_pred_path = OUTPUT_DIR / OUTPUT_FILES['train_predictions']\n",
    "    train_predictions.to_csv(train_pred_path, index=False)\n",
    "    print(f\"✓ {OUTPUT_FILES['train_predictions']}\")\n",
    "    \n",
    "    # Validation predictions\n",
    "    val_predictions = pd.DataFrame({\n",
    "        'actual': y_val.values,\n",
    "        'predicted': y_val_proba.argmax(axis=1),\n",
    "        'prob_down': y_val_proba[:, 0],\n",
    "        'prob_neutral': y_val_proba[:, 1],\n",
    "        'prob_up': y_val_proba[:, 2]\n",
    "    })\n",
    "    val_pred_path = OUTPUT_DIR / OUTPUT_FILES['val_predictions']\n",
    "    val_predictions.to_csv(val_pred_path, index=False)\n",
    "    print(f\"✓ {OUTPUT_FILES['val_predictions']}\")\n",
    "    \n",
    "    # Test predictions\n",
    "    test_predictions = pd.DataFrame({\n",
    "        'actual': y_test.values,\n",
    "        'predicted': y_test_proba.argmax(axis=1),\n",
    "        'prob_down': y_test_proba[:, 0],\n",
    "        'prob_neutral': y_test_proba[:, 1],\n",
    "        'prob_up': y_test_proba[:, 2]\n",
    "    })\n",
    "    test_pred_path = OUTPUT_DIR / OUTPUT_FILES['test_predictions']\n",
    "    test_predictions.to_csv(test_pred_path, index=False)\n",
    "    print(f\"✓ {OUTPUT_FILES['test_predictions']} (Day 5 - ready for agent!)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Feature importance analysis\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    feature_importance = model.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    for idx, row in importance_df.head(10).iterrows():\n",
    "        print(f\"{row['feature']:30s} {row['importance']:.6f}\")\n",
    "    \n",
    "    importance_path = OUTPUT_DIR / OUTPUT_FILES['feature_importance']\n",
    "    importance_df.to_csv(importance_path, index=False)\n",
    "    print(f\"\\n✓ {OUTPUT_FILES['feature_importance']}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"FINAL MODEL TRAINING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_split_data()\n",
    "    \n",
    "    # Load best parameters\n",
    "    best_params = load_best_params()\n",
    "    \n",
    "    # Train final model\n",
    "    model, results, y_train_proba, y_val_proba, y_test_proba = train_final_model(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # Load selected features\n",
    "    features_path = OUTPUT_DIR / OUTPUT_FILES['selected_features']\n",
    "    selected_features = joblib.load(features_path)\n",
    "    \n",
    "    # Save everything\n",
    "    save_model_and_predictions(\n",
    "        model, results,\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        y_train_proba, y_val_proba, y_test_proba,\n",
    "        selected_features\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL MODEL TRAINING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nModel Performance Summary:\")\n",
    "    print(f\"  Test Accuracy: {results['test']['accuracy']:.4f}\")\n",
    "    print(f\"  Test F1 Score: {results['test']['f1']:.4f}\")\n",
    "    print(f\"  Features Used: {len(selected_features)}\")\n",
    "    print(f\"\\nAll files saved to: {OUTPUT_DIR}\")\n",
    "    print(f\"\\nKey output: {OUTPUT_FILES['test_predictions']} (ready for agent!)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
