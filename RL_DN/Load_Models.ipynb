{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c45c88-1b71-4519-81e1-b2ff35734c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all models...\n",
      "\n",
      "Loading XGBoost...\n",
      "‚úì XGBoost loaded\n",
      "Loading LSTM...\n",
      "‚úì LSTM loaded (input=22, hidden=32)\n",
      "Loading TCN...\n",
      "‚úì TCN loaded (input=22, channels=[32, 64, 64])\n",
      "Loading Transformer...\n",
      "‚úì Transformer loaded (d_model=128, nhead=8, layers=4)\n",
      "\n",
      "============================================================\n",
      "‚úÖ ALL 4 MODELS LOADED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "Models ready for inference:\n",
      "1. xgb_model: XGBoost Booster\n",
      "2. lstm_model: LSTM (input=22, hidden=32, output=3)\n",
      "3. tcn_model: TCN (input=22, channels=[32,64,64], output=3)\n",
      "4. transformer_model: Transformer (d_model=128, heads=8, output=3)\n",
      "\n",
      "üéâ All models are in eval() mode and ready for predictions!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load all 4 trained models - FINAL WORKING VERSION\n",
    "\"\"\"\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Define LSTM model architecture with 2 FC layers\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, fc1_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc1(lstm_out[:, -1, :])\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Define TCN model architecture\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        padding = (kernel_size - 1) * dilation\n",
    "        self.conv1 = nn.Module()\n",
    "        self.conv1.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)\n",
    "        \n",
    "        self.conv2 = nn.Module()\n",
    "        self.conv2.conv = nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding, dilation=dilation)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv1.conv(x))\n",
    "        out = self.relu(self.conv2.conv(out))\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size, output_size=3):\n",
    "        super(TCNModel, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation = 2 ** i\n",
    "            in_ch = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_ch = num_channels[i]\n",
    "            layers.append(TemporalBlock(in_ch, out_ch, kernel_size, dilation))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.network(x)\n",
    "        out = out[:, :, -1]\n",
    "        return self.fc(out)\n",
    "\n",
    "# Define Transformer model architecture\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, dim_feedforward, fc1_size, output_size, max_len=5000):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc1 = nn.Linear(d_model, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "print(\"Loading all models...\\n\")\n",
    "\n",
    "try:\n",
    "    # 1. XGBoost\n",
    "    print(\"Loading XGBoost...\")\n",
    "    xgb_model = xgb.Booster()\n",
    "    xgb_model.load_model(\"C:/Users/wdkal/Downloads/RL_Model/xgb_best_model.json\")\n",
    "    print(\"‚úì XGBoost loaded\")\n",
    "    \n",
    "    # 2. LSTM\n",
    "    print(\"Loading LSTM...\")\n",
    "    lstm_state = torch.load(\"C:/Users/wdkal/Downloads/RL_Model/lstm_best_model.pt\", \n",
    "                            map_location=torch.device('cpu'))\n",
    "    \n",
    "    weight_ih = lstm_state['lstm.weight_ih_l0']\n",
    "    input_size = weight_ih.shape[1]\n",
    "    hidden_size = weight_ih.shape[0] // 4\n",
    "    num_layers = 2\n",
    "    fc1_size = lstm_state['fc1.weight'].shape[0]\n",
    "    output_size = lstm_state['fc2.weight'].shape[0]\n",
    "    \n",
    "    lstm_model = LSTMModel(input_size, hidden_size, num_layers, fc1_size, output_size)\n",
    "    lstm_model.load_state_dict(lstm_state)\n",
    "    lstm_model.eval()\n",
    "    print(f\"‚úì LSTM loaded (input={input_size}, hidden={hidden_size})\")\n",
    "    \n",
    "    # 3. TCN\n",
    "    print(\"Loading TCN...\")\n",
    "    tcn_state = torch.load(\"C:/Users/wdkal/Downloads/RL_Model/tcn_best_model.pt\",\n",
    "                           map_location=torch.device('cpu'))\n",
    "    \n",
    "    first_conv_weight = tcn_state['network.0.conv1.conv.weight']\n",
    "    num_inputs = first_conv_weight.shape[1]\n",
    "    kernel_size = first_conv_weight.shape[2]\n",
    "    \n",
    "    num_channels = []\n",
    "    i = 0\n",
    "    while f'network.{i}.conv1.conv.weight' in tcn_state:\n",
    "        num_channels.append(tcn_state[f'network.{i}.conv1.conv.weight'].shape[0])\n",
    "        i += 1\n",
    "    \n",
    "    output_size = tcn_state['fc.weight'].shape[0] if 'fc.weight' in tcn_state else 3\n",
    "    \n",
    "    tcn_model = TCNModel(num_inputs, num_channels, kernel_size, output_size=output_size)\n",
    "    tcn_model.load_state_dict(tcn_state)\n",
    "    tcn_model.eval()\n",
    "    print(f\"‚úì TCN loaded (input={num_inputs}, channels={num_channels})\")\n",
    "    \n",
    "    # 4. Transformer - Extract from checkpoint\n",
    "    print(\"Loading Transformer...\")\n",
    "    checkpoint = torch.load(\"C:/Users/wdkal/Downloads/RL_Model/best_transformer_model.pth\",\n",
    "                           map_location=torch.device('cpu'), weights_only=False)\n",
    "    \n",
    "    transformer_state = checkpoint['model_state_dict']\n",
    "    hyperparams = checkpoint['hyperparameters']\n",
    "    \n",
    "    # Get all parameters\n",
    "    input_size = hyperparams['input_size']\n",
    "    d_model = hyperparams['d_model']\n",
    "    nhead = hyperparams['nhead']\n",
    "    num_layers = hyperparams['num_layers']\n",
    "    dim_feedforward = hyperparams['dim_feedforward']\n",
    "    \n",
    "    # Infer fc sizes from weights\n",
    "    fc1_size = transformer_state['fc1.weight'].shape[0]\n",
    "    output_size = transformer_state['fc2.weight'].shape[0]\n",
    "    \n",
    "    # Infer max_len from positional encoding\n",
    "    max_len = transformer_state['pos_encoder.pe'].shape[1]\n",
    "    \n",
    "    transformer_model = TransformerModel(\n",
    "        input_size, d_model, nhead, num_layers, \n",
    "        dim_feedforward, fc1_size, output_size, max_len\n",
    "    )\n",
    "    transformer_model.load_state_dict(transformer_state)\n",
    "    transformer_model.eval()\n",
    "    print(f\"‚úì Transformer loaded (d_model={d_model}, nhead={nhead}, layers={num_layers})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ ALL 4 MODELS LOADED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nModels ready for inference:\")\n",
    "    print(f\"1. xgb_model: XGBoost Booster\")\n",
    "    print(f\"2. lstm_model: LSTM (input=22, hidden=32, output=3)\")\n",
    "    print(f\"3. tcn_model: TCN (input=22, channels=[32,64,64], output=3)\")\n",
    "    print(f\"4. transformer_model: Transformer (d_model={d_model}, heads={nhead}, output={output_size})\")\n",
    "    print(\"\\nüéâ All models are in eval() mode and ready for predictions!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f636dc75-b006-4d8f-a42f-fa7d430f7115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
