{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76569a-d06d-49f0-895e-063df000b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OPTIMIZED: Train Double DQN RL Agent with Episode Length Limits\n",
    "Key changes:\n",
    "1. Added max_steps_per_episode parameter\n",
    "2. Added progress tracking within episodes\n",
    "3. Reduced default episodes but each completes faster\n",
    "4. Added early stopping\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "RL_INPUT_PATH = Path(\"C:/Users/wdkal/Downloads/RL_INPUTS\")\n",
    "OUTPUT_PATH = Path(\"C:/Users/wdkal/Downloads/RL_OUTPUTS\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# OPTIMIZED TRADING ENVIRONMENT\n",
    "# ============================================================\n",
    "\n",
    "class TradingEnvironment:\n",
    "    \"\"\"Trading environment with episode length limits\"\"\"\n",
    "    def __init__(self, rl_input_path, transaction_cost=0.001, max_steps_per_episode=5000):\n",
    "        with open(rl_input_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        self.xgb_preds = data['predictions']['xgb']\n",
    "        self.lstm_preds = data['predictions']['lstm']\n",
    "        self.tcn_preds = data['predictions']['tcn']\n",
    "        self.transformer_preds = data['predictions']['transformer']\n",
    "        self.actual_labels = data['actual_labels']\n",
    "        \n",
    "        self.num_events = len(self.actual_labels)\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        \n",
    "        print(f\"  Loaded {self.num_events:,} events\")\n",
    "        print(f\"  Max steps per episode: {self.max_steps_per_episode:,}\")\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, random_start=True):\n",
    "        \"\"\"Reset environment with optional random starting point\"\"\"\n",
    "        if random_start and self.num_events > self.max_steps_per_episode:\n",
    "            # Start at random position to see different market conditions\n",
    "            max_start = self.num_events - self.max_steps_per_episode\n",
    "            self.start_step = random.randint(0, max_start)\n",
    "        else:\n",
    "            self.start_step = 0\n",
    "        \n",
    "        self.current_step = self.start_step\n",
    "        self.episode_step = 0  # Steps within this episode\n",
    "        self.position = 0.0\n",
    "        self.pnl = 0.0\n",
    "        self.pnl_history = [0.0]\n",
    "        self.trades = []\n",
    "        self.position_history = [0.0]\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Get current state (28 features)\"\"\"\n",
    "        if self.current_step >= self.num_events:\n",
    "            return None\n",
    "        \n",
    "        # Model predictions (12 features)\n",
    "        xgb = self.xgb_preds[self.current_step]\n",
    "        lstm = self.lstm_preds[self.current_step]\n",
    "        tcn = self.tcn_preds[self.current_step]\n",
    "        transformer = self.transformer_preds[self.current_step]\n",
    "        \n",
    "        # Ensemble stats (4 features)\n",
    "        all_probs = np.array([xgb, lstm, tcn, transformer])\n",
    "        mean_up = np.mean(all_probs[:, 2])\n",
    "        mean_neutral = np.mean(all_probs[:, 1])\n",
    "        mean_down = np.mean(all_probs[:, 0])\n",
    "        std_up = np.std(all_probs[:, 2])\n",
    "        \n",
    "        # Market features (5 features)\n",
    "        price_trend = mean_up - mean_down\n",
    "        confidence = max(mean_up, mean_neutral, mean_down)\n",
    "        uncertainty = std_up\n",
    "        non_neutral = mean_up + mean_down\n",
    "        time_progress = float(self.episode_step) / self.max_steps_per_episode\n",
    "        \n",
    "        # Trading state (7 features)\n",
    "        num_recent_trades = len([t for t in self.trades if t['step'] > self.episode_step - 100])\n",
    "        recent_pnl = self.pnl_history[-1]\n",
    "        \n",
    "        state = np.concatenate([\n",
    "            xgb, lstm, tcn, transformer,  # 12\n",
    "            [mean_up, mean_neutral, mean_down, std_up],  # 4\n",
    "            [price_trend, confidence, uncertainty, non_neutral, time_progress],  # 5\n",
    "            [self.position, recent_pnl/100, num_recent_trades/10, \n",
    "             self.episode_step/self.max_steps_per_episode, abs(self.position),\n",
    "             1.0 if self.position > 0 else 0.0, 1.0 if self.position < 0 else 0.0]  # 7\n",
    "        ])\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take action and return next state, reward, done\"\"\"\n",
    "        # Action mapping\n",
    "        action_map = {0: -0.5, 1: -0.3, 2: -0.1, 3: 0.0, 4: 0.1, 5: 0.3, 6: 0.5}\n",
    "        position_change = action_map[action]\n",
    "        \n",
    "        old_position = self.position\n",
    "        new_position = np.clip(self.position + position_change, -1.0, 1.0)\n",
    "        actual_change = new_position - old_position\n",
    "        \n",
    "        # Transaction cost\n",
    "        cost = abs(actual_change) * self.transaction_cost * 100\n",
    "        \n",
    "        self.position = new_position\n",
    "        self.position_history.append(self.position)\n",
    "        self.current_step += 1\n",
    "        self.episode_step += 1\n",
    "        \n",
    "        # Check if episode should end\n",
    "        done = (self.episode_step >= self.max_steps_per_episode or \n",
    "                self.current_step >= self.num_events)\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if self.current_step < self.num_events:\n",
    "            # Price movement\n",
    "            actual_label = self.actual_labels[self.current_step]\n",
    "            price_movement = (actual_label - 1) * 0.5\n",
    "            \n",
    "            # PnL calculation\n",
    "            pnl = self.position * price_movement * 100\n",
    "            reward = pnl - cost - 0.1 * (self.position ** 2)\n",
    "            \n",
    "            self.pnl += reward\n",
    "            self.pnl_history.append(self.pnl)\n",
    "            \n",
    "            # Track trades\n",
    "            if actual_change != 0:\n",
    "                self.trades.append({\n",
    "                    'step': self.episode_step,\n",
    "                    'action': action,\n",
    "                    'position': self.position,\n",
    "                    'pnl': reward\n",
    "                })\n",
    "        \n",
    "        next_state = self.get_state() if not done else None\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Calculate performance metrics\"\"\"\n",
    "        if len(self.pnl_history) < 2:\n",
    "            return {'total_pnl': 0, 'sharpe': 0, 'num_trades': 0, 'max_drawdown': 0}\n",
    "        \n",
    "        total_pnl = self.pnl_history[-1]\n",
    "        returns = np.diff(self.pnl_history)\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns) if len(returns) > 1 else 1\n",
    "        \n",
    "        sharpe = (mean_return / std_return) * np.sqrt(len(returns)) if std_return > 0 else 0\n",
    "        \n",
    "        # Max drawdown\n",
    "        cummax = np.maximum.accumulate(self.pnl_history)\n",
    "        drawdown = np.array(self.pnl_history) - cummax\n",
    "        max_drawdown = np.min(drawdown)\n",
    "        \n",
    "        return {\n",
    "            'total_pnl': total_pnl,\n",
    "            'num_trades': len(self.trades),\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_drawdown\n",
    "        }\n",
    "\n",
    "# ============================================================\n",
    "# DOUBLE DQN AGENT (Same as before)\n",
    "# ============================================================\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "class DoubleDQN:\n",
    "    def __init__(self, state_dim=28, action_dim=7, learning_rate=0.001,\n",
    "                 gamma=0.95, epsilon_start=1.0, epsilon_end=0.01,\n",
    "                 epsilon_decay=0.995, buffer_size=10000):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.policy_net = QNetwork(state_dim, action_dim)\n",
    "        self.target_net = QNetwork(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.losses = []\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train_step(self, batch_size=64):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(np.array([s if s is not None else np.zeros(self.state_dim) \n",
    "                                                    for s in next_states]))\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_states).argmax(1)\n",
    "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        loss = self.criterion(current_q.squeeze(), target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING WITH PROGRESS TRACKING\n",
    "# ============================================================\n",
    "\n",
    "def validate(agent, env):\n",
    "    \"\"\"Run validation episode\"\"\"\n",
    "    state = env.reset(random_start=False)  # Always start from beginning for validation\n",
    "    while state is not None:\n",
    "        action = agent.select_action(state, training=False)\n",
    "        state, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    return env.get_metrics()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIMIZED RL AGENT TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load environments with episode limits\n",
    "print(\"\\nLoading data...\")\n",
    "MAX_STEPS = 5000  # Limit each episode to 5000 steps for faster training\n",
    "train_env = TradingEnvironment(RL_INPUT_PATH / \"rl_input_train.pkl\", \n",
    "                               max_steps_per_episode=MAX_STEPS)\n",
    "val_env = TradingEnvironment(RL_INPUT_PATH / \"rl_input_val.pkl\",\n",
    "                            max_steps_per_episode=MAX_STEPS)\n",
    "\n",
    "# Create agent\n",
    "print(\"\\nCreating agent...\")\n",
    "agent = DoubleDQN(state_dim=28, action_dim=7, learning_rate=0.0005, \n",
    "                 gamma=0.95, epsilon_start=1.0, epsilon_end=0.01, \n",
    "                 epsilon_decay=0.995, buffer_size=50000)\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 200  # Reduced since each episode is now faster\n",
    "target_update_freq = 100\n",
    "batch_size = 64\n",
    "validate_freq = 5  # Validate more frequently\n",
    "early_stop_patience = 20\n",
    "\n",
    "best_val_sharpe = -np.inf\n",
    "no_improvement_count = 0\n",
    "train_rewards = []\n",
    "val_sharpes = []\n",
    "val_pnls = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Episodes: {num_episodes}\")\n",
    "print(f\"Max steps per episode: {MAX_STEPS:,}\")\n",
    "print(f\"Target update frequency: every {target_update_freq} steps\")\n",
    "print(f\"Validation frequency: every {validate_freq} episodes\")\n",
    "print(f\"Early stopping patience: {early_stop_patience} validations\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    episode_start = datetime.now()\n",
    "    state = train_env.reset(random_start=True)\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # Run episode with progress tracking\n",
    "    while state is not None:\n",
    "        action = agent.select_action(state, training=True)\n",
    "        next_state, reward, done, _ = train_env.step(action)\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Train\n",
    "        if len(agent.replay_buffer) >= batch_size:\n",
    "            agent.train_step(batch_size=batch_size)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if steps % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "    \n",
    "    # Decay exploration\n",
    "    agent.decay_epsilon()\n",
    "    train_rewards.append(episode_reward)\n",
    "    train_metrics = train_env.get_metrics()\n",
    "    \n",
    "    episode_time = (datetime.now() - episode_start).total_seconds()\n",
    "    \n",
    "    # Validate periodically\n",
    "    if episode % validate_freq == 0:\n",
    "        val_metrics = validate(agent, val_env)\n",
    "        val_sharpes.append(val_metrics['sharpe'])\n",
    "        val_pnls.append(val_metrics['total_pnl'])\n",
    "        \n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\nEpisode {episode}/{num_episodes} (took {episode_time:.1f}s, total: {elapsed/60:.1f}min)\")\n",
    "        print(f\"  Train - PnL: ${train_metrics['total_pnl']:.2f}, Sharpe: {train_metrics['sharpe']:.2f}, \"\n",
    "              f\"Trades: {train_metrics['num_trades']}, Steps: {steps}\")\n",
    "        print(f\"  Val   - PnL: ${val_metrics['total_pnl']:.2f}, Sharpe: {val_metrics['sharpe']:.2f}, \"\n",
    "              f\"Trades: {val_metrics['num_trades']}\")\n",
    "        print(f\"  Epsilon: {agent.epsilon:.3f}, Avg Loss: {np.mean(agent.losses[-100:]) if len(agent.losses) > 0 else 0:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['sharpe'] > best_val_sharpe:\n",
    "            best_val_sharpe = val_metrics['sharpe']\n",
    "            no_improvement_count = 0\n",
    "            agent.save(OUTPUT_PATH / \"best_agent.pt\")\n",
    "            print(f\"  âœ“ New best model! (Sharpe: {best_val_sharpe:.2f})\")\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            if no_improvement_count >= early_stop_patience:\n",
    "                print(f\"\\nâš  Early stopping: No improvement for {early_stop_patience} validations\")\n",
    "                break\n",
    "\n",
    "total_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nâ± Total training time: {total_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================================\n",
    "# PLOT RESULTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PLOTTING TRAINING PROGRESS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training rewards\n",
    "axes[0, 0].plot(train_rewards)\n",
    "axes[0, 0].set_title('Training Rewards per Episode')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Validation Sharpe\n",
    "val_episodes = list(range(0, len(train_rewards), validate_freq))[:len(val_sharpes)]\n",
    "axes[0, 1].plot(val_episodes, val_sharpes, marker='o')\n",
    "axes[0, 1].set_title('Validation Sharpe Ratio')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Sharpe Ratio')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Validation PnL\n",
    "axes[1, 0].plot(val_episodes, val_pnls, marker='o')\n",
    "axes[1, 0].set_title('Validation PnL')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('PnL ($)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Training loss\n",
    "if len(agent.losses) > 0:\n",
    "    window = 50\n",
    "    if len(agent.losses) > window:\n",
    "        smoothed = np.convolve(agent.losses, np.ones(window)/window, mode='valid')\n",
    "        axes[1, 1].plot(smoothed)\n",
    "    else:\n",
    "        axes[1, 1].plot(agent.losses)\n",
    "    axes[1, 1].set_title('Training Loss (smoothed)')\n",
    "    axes[1, 1].set_xlabel('Training Step')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'training_progress.png', dpi=150)\n",
    "print(f\"\\nâœ“ Saved training plots to {OUTPUT_PATH / 'training_progress.png'}\")\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest validation Sharpe: {best_val_sharpe:.2f}\")\n",
    "print(f\"Final training reward: ${train_rewards[-1]:.2f}\")\n",
    "print(f\"Total episodes: {len(train_rewards)}\")\n",
    "print(f\"Training time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Model saved to: {OUTPUT_PATH / 'best_agent.pt'}\")\n",
    "print(\"\\nðŸŽ‰ Ready for Step 4: Testing on Days 14-15!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
