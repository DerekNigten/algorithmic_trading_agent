{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bdc6692-bdab-4011-a233-1fcb025875ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "XGBOOST BALANCED PIPELINE\n",
      "Fast Two-Stage Tuning (~30-45 min total)\n",
      "============================================================\n",
      "============================================================\n",
      "STEP 1: LOADING RAW ORDER BOOK DATA\n",
      "============================================================\n",
      "  Loading 20251020...\n",
      "  Loading 20251021...\n",
      "  Loading 20251022...\n",
      "  Loading 20251023...\n",
      "  Loading 20251024...\n",
      "Total events loaded: 5,583,093\n",
      "\n",
      "============================================================\n",
      "STEP 2: CREATING FEATURES\n",
      "============================================================\n",
      "Created 37 features (excluding 'date')\n",
      "\n",
      "Creating labels (horizon = 23 events ahead)...\n",
      "\n",
      "============================================================\n",
      "STEP 3: FEATURE SELECTION\n",
      "============================================================\n",
      "Events ahead for MI calculation: 10\n",
      "Max correlation threshold: 0.85\n",
      "\n",
      "Computing Mutual Information scores...\n",
      "Initial features: 37\n",
      "\n",
      "Iterative correlation-based removal:\n",
      "  Iter 1: Removing 'ema_slow' (corr=1.000 with 'ema_fast', MI=0.0406)\n",
      "  Iter 2: Removing 'mid_diff' (corr=0.999 with 'mid_return', MI=0.0194)\n",
      "  Iter 3: Removing 'mid_price' (corr=0.999 with 'microprice', MI=0.0105)\n",
      "  Iter 4: Removing 'mv_1s' (corr=0.996 with 'ema_fast', MI=0.0144)\n",
      "  Iter 5: Removing 'ema_fast' (corr=0.996 with 'microprice', MI=0.0609)\n",
      "  Iter 6: Removing 'ASK_PRICE_1' (corr=0.996 with 'microprice', MI=0.0126)\n",
      "  Iter 7: Removing 'BID_PRICE_1' (corr=0.994 with 'microprice', MI=0.0096)\n",
      "  Iter 8: Removing 'ASK_PRICE_2' (corr=0.990 with 'ask_price_mean', MI=0.0131)\n",
      "  Iter 9: Removing 'BID_PRICE_2' (corr=0.988 with 'bid_price_mean', MI=0.0108)\n",
      "  Iter 10: Removing 'mv_5s' (corr=0.986 with 'microprice', MI=0.0280)\n",
      "  Iter 11: Removing 'BID_PRICE_3' (corr=0.980 with 'bid_price_mean', MI=0.0132)\n",
      "  Iter 12: Removing 'spread' (corr=0.977 with 'bid_ask_spread_ratio', MI=0.0166)\n",
      "  Iter 13: Removing 'ask_price_mean' (corr=0.958 with 'microprice', MI=0.0191)\n",
      "  Iter 14: Removing 'bid_price_mean' (corr=0.949 with 'microprice', MI=0.0182)\n",
      "  Iter 15: Removing 'ASK_PRICE_3' (corr=0.869 with 'microprice', MI=0.0145)\n",
      "\n",
      "Final features: 22\n",
      "Removed: 15 features\n",
      "\n",
      "============================================================\n",
      "SELECTED FEATURES WITH MI SCORES (sorted by MI)\n",
      "============================================================\n",
      "vol_5s                         0.434409\n",
      "vol_1s                         0.409058\n",
      "vol_100                        0.386024\n",
      "time_delta                     0.266916\n",
      "vol_10                         0.148954\n",
      "microprice                     0.085738\n",
      "rsi_14                         0.075094\n",
      "ASK_SIZE_1                     0.071756\n",
      "BID_SIZE_1                     0.067688\n",
      "bid_ask_spread_ratio           0.061342\n",
      "OFI                            0.058603\n",
      "ema_diff                       0.046423\n",
      "vol_imbalance                  0.045385\n",
      "ASK_SIZE_2                     0.044227\n",
      "BID_SIZE_2                     0.041095\n",
      "mid_return                     0.027424\n",
      "ASK_SIZE_3                     0.026284\n",
      "BID_SIZE_3                     0.023219\n",
      "ask_qty_mean                   0.017199\n",
      "bid_qty_mean                   0.014950\n",
      "price_cum_diff                 0.009619\n",
      "qty_cum_diff                   0.007273\n",
      "\n",
      "============================================================\n",
      "STEP 4: DATA SPLITTING AND NORMALIZATION\n",
      "============================================================\n",
      "Train: 3,890,472 events (days 20251020, 20251021, 20251022)\n",
      "Val:   762,279 events (days 20251023)\n",
      "Test:  930,342 events (days 20251024)\n",
      "\n",
      "Normalizing features using StandardScaler...\n",
      "\n",
      "Label Distribution:\n",
      "\n",
      "Train set (3,890,472 samples):\n",
      "  Down (0):    1,803,925 (46.37%)\n",
      "  Neutral (1): 287,038 ( 7.38%)\n",
      "  Up (2):      1,799,509 (46.25%)\n",
      "\n",
      "Val set (762,279 samples):\n",
      "  Down (0):    345,008 (45.26%)\n",
      "  Neutral (1): 69,472 ( 9.11%)\n",
      "  Up (2):      347,799 (45.63%)\n",
      "\n",
      "Test set (930,342 samples):\n",
      "  Down (0):    441,709 (47.48%)\n",
      "  Neutral (1): 54,126 ( 5.82%)\n",
      "  Up (2):      434,507 (46.70%)\n",
      "\n",
      "============================================================\n",
      "STEP 5: TWO-STAGE HYPERPARAMETER TUNING (BALANCED)\n",
      "============================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "STAGE 1: COARSE GRID SEARCH (REDUCED)\n",
      "------------------------------------------------------------\n",
      "Testing 32 combinations...\n",
      "\n",
      "[ 1/32] d=4, lr=0.05, n=150, mcw=1, g=0.0 | Acc=0.6014, F1=0.5545\n",
      "[ 2/32] d=4, lr=0.05, n=150, mcw=1, g=0.1 | Acc=0.6014, F1=0.5545\n",
      "[ 3/32] d=4, lr=0.05, n=150, mcw=5, g=0.0 | Acc=0.6014, F1=0.5545\n",
      "[ 4/32] d=4, lr=0.05, n=150, mcw=5, g=0.1 | Acc=0.6014, F1=0.5545\n",
      "[ 5/32] d=4, lr=0.05, n=250, mcw=1, g=0.0 | Acc=0.6040, F1=0.5640\n",
      "[ 6/32] d=4, lr=0.05, n=250, mcw=1, g=0.1 | Acc=0.6040, F1=0.5640\n",
      "[ 7/32] d=4, lr=0.05, n=250, mcw=5, g=0.0 | Acc=0.6040, F1=0.5643\n",
      "[ 8/32] d=4, lr=0.05, n=250, mcw=5, g=0.1 | Acc=0.6040, F1=0.5643\n",
      "[ 9/32] d=4, lr=0.15, n=150, mcw=1, g=0.0 | Acc=0.6069, F1=0.5756\n",
      "[10/32] d=4, lr=0.15, n=150, mcw=1, g=0.1 | Acc=0.6069, F1=0.5756\n",
      "[11/32] d=4, lr=0.15, n=150, mcw=5, g=0.0 | Acc=0.6070, F1=0.5757\n",
      "[12/32] d=4, lr=0.15, n=150, mcw=5, g=0.1 | Acc=0.6070, F1=0.5757\n",
      "[13/32] d=4, lr=0.15, n=250, mcw=1, g=0.0 | Acc=0.6079, F1=0.5799\n",
      "[14/32] d=4, lr=0.15, n=250, mcw=1, g=0.1 | Acc=0.6079, F1=0.5799\n",
      "[15/32] d=4, lr=0.15, n=250, mcw=5, g=0.0 | Acc=0.6077, F1=0.5798\n",
      "[16/32] d=4, lr=0.15, n=250, mcw=5, g=0.1 | Acc=0.6077, F1=0.5798\n",
      "[17/32] d=7, lr=0.05, n=150, mcw=1, g=0.0 | Acc=0.6103, F1=0.5841\n",
      "[18/32] d=7, lr=0.05, n=150, mcw=1, g=0.1 | Acc=0.6108, F1=0.5846\n",
      "[19/32] d=7, lr=0.05, n=150, mcw=5, g=0.0 | Acc=0.6107, F1=0.5846\n",
      "[20/32] d=7, lr=0.05, n=150, mcw=5, g=0.1 | Acc=0.6107, F1=0.5846\n",
      "[21/32] d=7, lr=0.05, n=250, mcw=1, g=0.0 | Acc=0.6106, F1=0.5868\n",
      "[22/32] d=7, lr=0.05, n=250, mcw=1, g=0.1 | Acc=0.6109, F1=0.5873\n",
      "[23/32] d=7, lr=0.05, n=250, mcw=5, g=0.0 | Acc=0.6108, F1=0.5869\n",
      "[24/32] d=7, lr=0.05, n=250, mcw=5, g=0.1 | Acc=0.6112, F1=0.5879\n",
      "[25/32] d=7, lr=0.15, n=150, mcw=1, g=0.0 | Acc=0.6116, F1=0.5890\n",
      "[26/32] d=7, lr=0.15, n=150, mcw=1, g=0.1 | Acc=0.6111, F1=0.5881\n",
      "[27/32] d=7, lr=0.15, n=150, mcw=5, g=0.0 | Acc=0.6110, F1=0.5887\n",
      "[28/32] d=7, lr=0.15, n=150, mcw=5, g=0.1 | Acc=0.6109, F1=0.5878\n",
      "[29/32] d=7, lr=0.15, n=250, mcw=1, g=0.0 | Acc=0.6116, F1=0.5890\n",
      "[30/32] d=7, lr=0.15, n=250, mcw=1, g=0.1 | Acc=0.6111, F1=0.5881\n",
      "[31/32] d=7, lr=0.15, n=250, mcw=5, g=0.0 | Acc=0.6110, F1=0.5887\n",
      "[32/32] d=7, lr=0.15, n=250, mcw=5, g=0.1 | Acc=0.6109, F1=0.5878\n",
      "\n",
      "============================================================\n",
      "STAGE 1 BEST: Acc=0.6116\n",
      "  max_depth: 7\n",
      "  learning_rate: 0.15\n",
      "  n_estimators: 150\n",
      "  min_child_weight: 1\n",
      "  gamma: 0\n",
      "\n",
      "------------------------------------------------------------\n",
      "STAGE 2: FINE-TUNING\n",
      "------------------------------------------------------------\n",
      "Testing 243 refined combinations...\n",
      "\n",
      "[ 1/243] d=6, lr=0.120, n=100, ss=0.75, cs=0.75 | Acc=0.6101, F1=0.5842\n",
      "[ 2/243] d=6, lr=0.120, n=100, ss=0.75, cs=0.80 | Acc=0.6103, F1=0.5837\n",
      "[ 6/243] d=6, lr=0.120, n=100, ss=0.80, cs=0.85 | Acc=0.6105, F1=0.5850\n",
      "[10/243] d=6, lr=0.120, n=150, ss=0.75, cs=0.75 | Acc=0.6096, F1=0.5852\n",
      "[15/243] d=6, lr=0.120, n=150, ss=0.80, cs=0.85 | Acc=0.6108, F1=0.5882\n",
      "[20/243] d=6, lr=0.120, n=200, ss=0.75, cs=0.80 | Acc=0.6102, F1=0.5852\n",
      "[30/243] d=6, lr=0.150, n=100, ss=0.75, cs=0.85 | Acc=0.6087, F1=0.5826\n",
      "[40/243] d=6, lr=0.150, n=150, ss=0.80, cs=0.75 | Acc=0.6096, F1=0.5854\n",
      "[50/243] d=6, lr=0.150, n=200, ss=0.80, cs=0.80 | Acc=0.6099, F1=0.5851\n",
      "[60/243] d=6, lr=0.180, n=100, ss=0.80, cs=0.85 | Acc=0.6097, F1=0.5859\n",
      "[70/243] d=6, lr=0.180, n=150, ss=0.85, cs=0.75 | Acc=0.6094, F1=0.5843\n",
      "[80/243] d=6, lr=0.180, n=200, ss=0.85, cs=0.80 | Acc=0.6101, F1=0.5847\n",
      "[85/243] d=7, lr=0.120, n=100, ss=0.80, cs=0.75 | Acc=0.6108, F1=0.5867\n",
      "[86/243] d=7, lr=0.120, n=100, ss=0.80, cs=0.80 | Acc=0.6114, F1=0.5876\n",
      "[90/243] d=7, lr=0.120, n=100, ss=0.85, cs=0.85 | Acc=0.6111, F1=0.5886\n",
      "[100/243] d=7, lr=0.120, n=200, ss=0.75, cs=0.75 | Acc=0.6104, F1=0.5868\n",
      "[110/243] d=7, lr=0.150, n=100, ss=0.75, cs=0.80 | Acc=0.6120, F1=0.5895\n",
      "[120/243] d=7, lr=0.150, n=150, ss=0.75, cs=0.85 | Acc=0.6117, F1=0.5893\n",
      "[130/243] d=7, lr=0.150, n=200, ss=0.80, cs=0.75 | Acc=0.6111, F1=0.5882\n",
      "[140/243] d=7, lr=0.180, n=100, ss=0.80, cs=0.80 | Acc=0.6106, F1=0.5898\n",
      "[150/243] d=7, lr=0.180, n=150, ss=0.80, cs=0.85 | Acc=0.6098, F1=0.5880\n",
      "[160/243] d=7, lr=0.180, n=200, ss=0.85, cs=0.75 | Acc=0.6103, F1=0.5871\n",
      "[170/243] d=8, lr=0.120, n=100, ss=0.85, cs=0.80 | Acc=0.6116, F1=0.5893\n",
      "[180/243] d=8, lr=0.120, n=150, ss=0.85, cs=0.85 | Acc=0.6112, F1=0.5894\n",
      "[190/243] d=8, lr=0.150, n=100, ss=0.75, cs=0.75 | Acc=0.6109, F1=0.5884\n",
      "[200/243] d=8, lr=0.150, n=150, ss=0.75, cs=0.80 | Acc=0.6115, F1=0.5902\n",
      "[210/243] d=8, lr=0.150, n=200, ss=0.75, cs=0.85 | Acc=0.6116, F1=0.5901\n",
      "[220/243] d=8, lr=0.180, n=100, ss=0.80, cs=0.75 | Acc=0.6114, F1=0.5894\n",
      "[230/243] d=8, lr=0.180, n=150, ss=0.80, cs=0.80 | Acc=0.6102, F1=0.5880\n",
      "[240/243] d=8, lr=0.180, n=200, ss=0.80, cs=0.85 | Acc=0.6103, F1=0.5887\n",
      "\n",
      "============================================================\n",
      "FINAL BEST PARAMETERS (Val Accuracy: 0.6120)\n",
      "============================================================\n",
      "  max_depth: 7\n",
      "  learning_rate: 0.1500\n",
      "  n_estimators: 100\n",
      "  subsample: 0.75\n",
      "  colsample_bytree: 0.80\n",
      "  min_child_weight: 1\n",
      "  gamma: 0\n",
      "\n",
      "============================================================\n",
      "STEP 6: TRAINING FINAL MODEL\n",
      "============================================================\n",
      "\n",
      "MODEL PERFORMANCE:\n",
      "============================================================\n",
      "Set        Accuracy     Precision    Recall       F1          \n",
      "------------------------------------------------------------\n",
      "Train      0.6248       0.6468       0.5510       0.5811\n",
      "Val        0.6120       0.6454       0.5610       0.5895\n",
      "Test       0.6089       0.6433       0.5245       0.5580\n",
      "\n",
      "============================================================\n",
      "DETAILED TEST SET CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Down (0)       0.62      0.60      0.61    441709\n",
      " Neutral (1)       0.72      0.32      0.44     54126\n",
      "      Up (2)       0.60      0.66      0.63    434507\n",
      "\n",
      "    accuracy                           0.61    930342\n",
      "   macro avg       0.64      0.52      0.56    930342\n",
      "weighted avg       0.61      0.61      0.61    930342\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[263385   3588 174736]\n",
      " [ 18544  17291  18291]\n",
      " [145427   3243 285837]]\n",
      "\n",
      "============================================================\n",
      "STEP 7: SAVING OUTPUTS\n",
      "============================================================\n",
      "Saving to: C:\\Users\\wdkal\\Downloads\\IE421_XGBOOST_DATA\n",
      "✓ xgb_best_model.json\n",
      "✓ xgb_training_history.json\n",
      "✓ selected_features.pkl\n",
      "✓ scaler.pkl\n",
      "✓ xgb_train_predictions.csv\n",
      "✓ xgb_val_predictions.csv\n",
      "✓ xgb_test_predictions.csv (Day 5 - ready for agent!)\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Model Performance Summary:\n",
      "  Test Accuracy: 0.6089\n",
      "  Test F1 Score: 0.5580\n",
      "  Features Used: 22\n",
      "\n",
      "All files saved to: C:\\Users\\wdkal\\Downloads\\IE421_XGBOOST_DATA\n",
      "\n",
      "Key output: xgb_test_predictions.csv (ready for agent!)\n",
      "\n",
      "============================================================\n",
      "TOP 10 MOST IMPORTANT FEATURES\n",
      "============================================================\n",
      "rsi_14                         0.153942\n",
      "vol_imbalance                  0.142789\n",
      "vol_10                         0.119307\n",
      "OFI                            0.089807\n",
      "BID_SIZE_1                     0.079323\n",
      "mid_return                     0.072687\n",
      "ema_diff                       0.061409\n",
      "ASK_SIZE_1                     0.060599\n",
      "bid_ask_spread_ratio           0.059013\n",
      "vol_100                        0.030799\n",
      "\n",
      "✓ feature_importance.csv saved\n",
      "\n",
      "============================================================\n",
      "TUNING SUMMARY\n",
      "============================================================\n",
      "Stage 1: Tested 32 combinations (coarse search)\n",
      "Stage 2: Tested 81 combinations (fine-tuning)\n",
      "Total combinations evaluated: 113\n",
      "\n",
      "Estimated runtime: 30-45 minutes\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "XGBoost Balanced Pipeline - Optimal Performance in ~30-45 minutes\n",
    "Two-stage tuning with reduced Stage 1 grid\n",
    "Output: C:/Users/wdkal/Downloads/IE421_XGBOOST_DATA/\n",
    "\n",
    "Run: python xgboost_balanced.py\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "RAW_DATA_PATH = Path(\"C:/Users/wdkal/iex_data/book_snapshots\")\n",
    "OUTPUT_DIR = Path(\"C:/Users/wdkal/Downloads/IE421_XGBOOST_DATA\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#####################################\n",
    "# 1. Load order book data\n",
    "#####################################\n",
    "def fetch_orderbook_data(dates=['20251020', '20251021', '20251022', '20251023', '20251024']):\n",
    "    \"\"\"Load IEX order book snapshots from multiple dates.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 1: LOADING RAW ORDER BOOK DATA\")\n",
    "    print(\"=\"*60)\n",
    "    dfs = []\n",
    "    for date in dates:\n",
    "        file_path = RAW_DATA_PATH / f'{date}_book_updates.csv.gz'\n",
    "        print(f\"  Loading {date}...\")\n",
    "        df = pd.read_csv(file_path, compression='gzip')\n",
    "        df['date'] = date\n",
    "        df['COLLECTION_TIME'] = pd.to_datetime(df['COLLECTION_TIME'])\n",
    "        df = df.set_index('COLLECTION_TIME')\n",
    "        df = df.between_time(\"14:30\", \"21:00\")\n",
    "        df = df.reset_index()\n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Total events loaded: {len(combined):,}\")\n",
    "    return combined\n",
    "\n",
    "#####################################\n",
    "# 2. Feature engineering\n",
    "#####################################\n",
    "def add_all_features(df):\n",
    "    \"\"\"Create comprehensive order book features.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: CREATING FEATURES\")\n",
    "    print(\"=\"*60)\n",
    "    features = pd.DataFrame()\n",
    "    features['date'] = df['date']\n",
    "    \n",
    "    # Basic Level-1 features\n",
    "    features[\"mid_price\"] = (df[\"BID_PRICE_1\"] + df[\"ASK_PRICE_1\"]) / 2\n",
    "    features[\"microprice\"] = (df[\"BID_PRICE_1\"] * df[\"ASK_SIZE_1\"] + \n",
    "                              df[\"ASK_PRICE_1\"] * df[\"BID_SIZE_1\"]) / (df[\"BID_SIZE_1\"] + df[\"ASK_SIZE_1\"] + 1e-10)\n",
    "    features[\"spread\"] = df[\"ASK_PRICE_1\"] - df[\"BID_PRICE_1\"]\n",
    "    features[\"vol_imbalance\"] = (df[\"BID_SIZE_1\"] - df[\"ASK_SIZE_1\"]) / (df[\"BID_SIZE_1\"] + df[\"ASK_SIZE_1\"] + 1e-6)\n",
    "    features[\"bid_ask_spread_ratio\"] = features[\"spread\"] / features[\"mid_price\"]\n",
    "    \n",
    "    # All level prices and sizes\n",
    "    features[\"BID_PRICE_1\"] = df[\"BID_PRICE_1\"]\n",
    "    features[\"BID_SIZE_1\"] = df[\"BID_SIZE_1\"]\n",
    "    features[\"BID_PRICE_2\"] = df[\"BID_PRICE_2\"]\n",
    "    features[\"BID_SIZE_2\"] = df[\"BID_SIZE_2\"]\n",
    "    features[\"BID_PRICE_3\"] = df[\"BID_PRICE_3\"]\n",
    "    features[\"BID_SIZE_3\"] = df[\"BID_SIZE_3\"]\n",
    "    features[\"ASK_PRICE_1\"] = df[\"ASK_PRICE_1\"]\n",
    "    features[\"ASK_SIZE_1\"] = df[\"ASK_SIZE_1\"]\n",
    "    features[\"ASK_PRICE_2\"] = df[\"ASK_PRICE_2\"]\n",
    "    features[\"ASK_SIZE_2\"] = df[\"ASK_SIZE_2\"]\n",
    "    features[\"ASK_PRICE_3\"] = df[\"ASK_PRICE_3\"]\n",
    "    features[\"ASK_SIZE_3\"] = df[\"ASK_SIZE_3\"]\n",
    "    \n",
    "    # Mean price/quantity across all 3 levels\n",
    "    features[\"bid_price_mean\"] = (df[\"BID_PRICE_1\"] + df[\"BID_PRICE_2\"] + df[\"BID_PRICE_3\"]) / 3\n",
    "    features[\"ask_price_mean\"] = (df[\"ASK_PRICE_1\"] + df[\"ASK_PRICE_2\"] + df[\"ASK_PRICE_3\"]) / 3\n",
    "    features[\"bid_qty_mean\"] = (df[\"BID_SIZE_1\"] + df[\"BID_SIZE_2\"] + df[\"BID_SIZE_3\"]) / 3\n",
    "    features[\"ask_qty_mean\"] = (df[\"ASK_SIZE_1\"] + df[\"ASK_SIZE_2\"] + df[\"ASK_SIZE_3\"]) / 3\n",
    "    \n",
    "    # Cumulative differences across 3 levels\n",
    "    features[\"price_cum_diff\"] = (df[\"ASK_PRICE_1\"] - df[\"BID_PRICE_1\"] + \n",
    "                                  df[\"ASK_PRICE_2\"] - df[\"BID_PRICE_2\"] + \n",
    "                                  df[\"ASK_PRICE_3\"] - df[\"BID_PRICE_3\"])\n",
    "    features[\"qty_cum_diff\"] = (df[\"ASK_SIZE_1\"] - df[\"BID_SIZE_1\"] + \n",
    "                                df[\"ASK_SIZE_2\"] - df[\"BID_SIZE_2\"] + \n",
    "                                df[\"ASK_SIZE_3\"] - df[\"BID_SIZE_3\"])\n",
    "    \n",
    "    # Time intervals between events\n",
    "    features[\"time_delta\"] = 1  # Placeholder\n",
    "    \n",
    "    # Price momentum\n",
    "    features[\"mid_diff\"] = features[\"mid_price\"].diff()\n",
    "    features[\"mid_return\"] = features[\"mid_diff\"] / features[\"mid_price\"].shift(1)\n",
    "    \n",
    "    # Order Flow Imbalance (OFI)\n",
    "    features[\"total_bid_qty\"] = df[\"BID_SIZE_1\"] + df[\"BID_SIZE_2\"] + df[\"BID_SIZE_3\"]\n",
    "    features[\"total_ask_qty\"] = df[\"ASK_SIZE_1\"] + df[\"ASK_SIZE_2\"] + df[\"ASK_SIZE_3\"]\n",
    "    features[\"bid_qty_change\"] = features[\"total_bid_qty\"].diff()\n",
    "    features[\"ask_qty_change\"] = features[\"total_ask_qty\"].diff()\n",
    "    features[\"OFI\"] = features[\"bid_qty_change\"] - features[\"ask_qty_change\"]\n",
    "    \n",
    "    # Moving averages\n",
    "    features[\"mv_1s\"] = features[\"mid_price\"].rolling(1000, min_periods=1).mean()\n",
    "    features[\"mv_5s\"] = features[\"mid_price\"].rolling(5000, min_periods=1).mean()\n",
    "    \n",
    "    # Volatility\n",
    "    features[\"vol_10\"] = features[\"mid_return\"].rolling(10, min_periods=1).std()\n",
    "    features[\"vol_100\"] = features[\"mid_return\"].rolling(100, min_periods=1).std()\n",
    "    features[\"vol_1s\"] = features[\"mid_return\"].rolling(1000, min_periods=1).std()\n",
    "    features[\"vol_5s\"] = features[\"mid_return\"].rolling(5000, min_periods=1).std()\n",
    "    \n",
    "    # RSI\n",
    "    delta = features[\"microprice\"].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(14, min_periods=1).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14, min_periods=1).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    features[\"rsi_14\"] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # EMA\n",
    "    features[\"ema_fast\"] = features[\"mid_price\"].ewm(span=12, adjust=False).mean()\n",
    "    features[\"ema_slow\"] = features[\"mid_price\"].ewm(span=26, adjust=False).mean()\n",
    "    features[\"ema_diff\"] = features[\"ema_fast\"] - features[\"ema_slow\"]\n",
    "    \n",
    "    # Clean up intermediate columns\n",
    "    features.drop(['total_bid_qty', 'total_ask_qty', 'bid_qty_change', 'ask_qty_change'], \n",
    "                  axis=1, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Fill NaN and Inf values\n",
    "    features = features.ffill().fillna(0)\n",
    "    features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    features = features.ffill().fillna(0)\n",
    "    \n",
    "    print(f\"Created {len(features.columns)-1} features (excluding 'date')\")\n",
    "    return features\n",
    "\n",
    "#####################################\n",
    "# 3. Add labels\n",
    "#####################################\n",
    "def add_labels(features, horizon=23):\n",
    "    \"\"\"Create price movement labels.\"\"\"\n",
    "    print(f\"\\nCreating labels (horizon = {horizon} events ahead)...\")\n",
    "    features['future_price'] = features['microprice'].shift(-horizon)\n",
    "    price_change = features['future_price'] - features['microprice']\n",
    "    \n",
    "    features['target'] = 1  # neutral (no change)\n",
    "    features.loc[price_change > 0, 'target'] = 2  # up\n",
    "    features.loc[price_change < 0, 'target'] = 0  # down\n",
    "    \n",
    "    features.drop('future_price', axis=1, inplace=True)\n",
    "    return features\n",
    "\n",
    "#####################################\n",
    "# 4. Feature selection\n",
    "#####################################\n",
    "def select_features(df, number_events_ahead=10, max_corr=0.85):\n",
    "    \"\"\"Select features using MI scores and Spearman correlation filtering.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: FEATURE SELECTION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Events ahead for MI calculation: {number_events_ahead}\")\n",
    "    print(f\"Max correlation threshold: {max_corr}\")\n",
    "    \n",
    "    # Create temporary label for feature selection\n",
    "    temp_df = df.copy()\n",
    "    temp_df[\"label\"] = 0\n",
    "    temp_df.loc[temp_df[\"mid_price\"].shift(-number_events_ahead) > temp_df[\"mid_price\"], \"label\"] = 1\n",
    "    temp_df.loc[temp_df[\"mid_price\"].shift(-number_events_ahead) < temp_df[\"mid_price\"], \"label\"] = -1\n",
    "\n",
    "    feature_names = [col for col in temp_df.columns if col not in ['target', 'date', 'label']]\n",
    "    temp_df = temp_df.dropna()\n",
    "    \n",
    "    # Compute MI scores\n",
    "    print(\"\\nComputing Mutual Information scores...\")\n",
    "    X = temp_df[feature_names].values\n",
    "    y = temp_df['label'].values\n",
    "    mi_scores = mutual_info_classif(X, y, random_state=42, n_neighbors=3)\n",
    "    mi_dict = dict(zip(feature_names, mi_scores))\n",
    "    \n",
    "    selected_features = feature_names.copy()\n",
    "    print(f\"Initial features: {len(selected_features)}\")\n",
    "    print(\"\\nIterative correlation-based removal:\")\n",
    "    \n",
    "    # Iterative removal\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        corr_matrix, _ = spearmanr(temp_df[selected_features].values)\n",
    "        max_correlation = -1\n",
    "        remove_feat = None\n",
    "        \n",
    "        for i in range(len(selected_features)):\n",
    "            for j in range(i+1, len(selected_features)):\n",
    "                corr_val = abs(corr_matrix[i, j])\n",
    "                \n",
    "                if corr_val > max_corr and corr_val > max_correlation:\n",
    "                    max_correlation = corr_val\n",
    "                    feat1 = selected_features[i]\n",
    "                    feat2 = selected_features[j]\n",
    "                    \n",
    "                    if mi_dict[feat1] < mi_dict[feat2]:\n",
    "                        remove_feat = feat1\n",
    "                        keep_feat = feat2\n",
    "                    else:\n",
    "                        remove_feat = feat2\n",
    "                        keep_feat = feat1\n",
    "        \n",
    "        if remove_feat is None:\n",
    "            break\n",
    "        \n",
    "        print(f\"  Iter {iteration}: Removing '{remove_feat}' (corr={max_correlation:.3f} with '{keep_feat}', MI={mi_dict[remove_feat]:.4f})\")\n",
    "        selected_features.remove(remove_feat)\n",
    "    \n",
    "    print(f\"\\nFinal features: {len(selected_features)}\")\n",
    "    print(f\"Removed: {len(feature_names) - len(selected_features)} features\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SELECTED FEATURES WITH MI SCORES (sorted by MI)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    selected_mi = [(feat, mi_dict[feat]) for feat in selected_features]\n",
    "    selected_mi.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for feat, mi_score in selected_mi:\n",
    "        print(f\"{feat:30s} {mi_score:.6f}\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "#####################################\n",
    "# 5. Normalize and split data\n",
    "#####################################\n",
    "def normalize_and_split(features, selected_features, train_dates, val_dates, test_dates):\n",
    "    \"\"\"Split by date and normalize features.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 4: DATA SPLITTING AND NORMALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    train_df = features[features['date'].isin(train_dates)].copy()\n",
    "    val_df = features[features['date'].isin(val_dates)].copy()\n",
    "    test_df = features[features['date'].isin(test_dates)].copy()\n",
    "    \n",
    "    print(f\"Train: {len(train_df):,} events (days {', '.join(train_dates)})\")\n",
    "    print(f\"Val:   {len(val_df):,} events (days {', '.join(val_dates)})\")\n",
    "    print(f\"Test:  {len(test_df):,} events (days {', '.join(test_dates)})\")\n",
    "    \n",
    "    train_df = train_df[selected_features + ['target']]\n",
    "    val_df = val_df[selected_features + ['target']]\n",
    "    test_df = test_df[selected_features + ['target']]\n",
    "    \n",
    "    print(\"\\nNormalizing features using StandardScaler...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df[selected_features])\n",
    "    \n",
    "    train_df[selected_features] = scaler.transform(train_df[selected_features])\n",
    "    val_df[selected_features] = scaler.transform(val_df[selected_features])\n",
    "    test_df[selected_features] = scaler.transform(test_df[selected_features])\n",
    "    \n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "        counts = df['target'].value_counts().sort_index()\n",
    "        total = len(df)\n",
    "        print(f\"\\n{name} set ({total:,} samples):\")\n",
    "        print(f\"  Down (0):    {counts.get(0, 0):6,} ({counts.get(0, 0)/total*100:5.2f}%)\")\n",
    "        print(f\"  Neutral (1): {counts.get(1, 0):6,} ({counts.get(1, 0)/total*100:5.2f}%)\")\n",
    "        print(f\"  Up (2):      {counts.get(2, 0):6,} ({counts.get(2, 0)/total*100:5.2f}%)\")\n",
    "    \n",
    "    X_train, y_train = train_df[selected_features], train_df['target']\n",
    "    X_val, y_val = val_df[selected_features], val_df['target']\n",
    "    X_test, y_test = test_df[selected_features], test_df['target']\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, scaler\n",
    "\n",
    "#####################################\n",
    "# 6. Two-stage hyperparameter tuning (OPTIMIZED)\n",
    "#####################################\n",
    "def tune_hyperparameters_two_stage(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Two-stage grid search - BALANCED FOR SPEED & PERFORMANCE.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 5: TWO-STAGE HYPERPARAMETER TUNING (BALANCED)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # STAGE 1: REDUCED Coarse search - 32 combinations (~10-15 min)\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STAGE 1: COARSE GRID SEARCH (REDUCED)\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    stage1_grid = {\n",
    "        'max_depth': [4, 7],                    # 2 options\n",
    "        'learning_rate': [0.05, 0.15],          # 2 options\n",
    "        'n_estimators': [150, 250],             # 2 options\n",
    "        'subsample': [0.8],                     # 1 option (fixed at good default)\n",
    "        'colsample_bytree': [0.8],              # 1 option (fixed at good default)\n",
    "        'min_child_weight': [1, 5],             # 2 options\n",
    "        'gamma': [0, 0.1],                      # 2 options\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    best_stage1_acc = 0\n",
    "    best_stage1_params = None\n",
    "    stage1_history = []\n",
    "    \n",
    "    total_stage1 = (len(stage1_grid['max_depth']) * len(stage1_grid['learning_rate']) * \n",
    "                    len(stage1_grid['n_estimators']) * len(stage1_grid['subsample']) * \n",
    "                    len(stage1_grid['colsample_bytree']) * len(stage1_grid['min_child_weight']) * \n",
    "                    len(stage1_grid['gamma']))\n",
    "    \n",
    "    print(f\"Testing {total_stage1} combinations...\\n\")\n",
    "    \n",
    "    current = 0\n",
    "    for max_depth in stage1_grid['max_depth']:\n",
    "        for lr in stage1_grid['learning_rate']:\n",
    "            for n_est in stage1_grid['n_estimators']:\n",
    "                for subsample in stage1_grid['subsample']:\n",
    "                    for colsample in stage1_grid['colsample_bytree']:\n",
    "                        for min_child in stage1_grid['min_child_weight']:\n",
    "                            for gamma in stage1_grid['gamma']:\n",
    "                                current += 1\n",
    "                                \n",
    "                                params = {\n",
    "                                    'objective': 'multi:softmax',\n",
    "                                    'num_class': 3,\n",
    "                                    'max_depth': max_depth,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'n_estimators': n_est,\n",
    "                                    'subsample': subsample,\n",
    "                                    'colsample_bytree': colsample,\n",
    "                                    'min_child_weight': min_child,\n",
    "                                    'gamma': gamma,\n",
    "                                    'random_state': stage1_grid['random_state'],\n",
    "                                    'eval_metric': 'mlogloss',\n",
    "                                    'early_stopping_rounds': 20\n",
    "                                }\n",
    "                                \n",
    "                                model = xgb.XGBClassifier(**params)\n",
    "                                model.fit(X_train, y_train, \n",
    "                                         eval_set=[(X_val, y_val)],\n",
    "                                         verbose=False)\n",
    "                                \n",
    "                                y_val_pred = model.predict(X_val)\n",
    "                                val_acc = accuracy_score(y_val, y_val_pred)\n",
    "                                precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                                    y_val, y_val_pred, average='macro', zero_division=0\n",
    "                                )\n",
    "                                \n",
    "                                print(f\"[{current:2d}/{total_stage1}] d={max_depth}, lr={lr:.2f}, n={n_est:3d}, \"\n",
    "                                      f\"mcw={min_child}, g={gamma:.1f} | Acc={val_acc:.4f}, F1={f1:.4f}\")\n",
    "                                \n",
    "                                stage1_history.append({\n",
    "                                    'max_depth': max_depth,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'n_estimators': n_est,\n",
    "                                    'subsample': subsample,\n",
    "                                    'colsample_bytree': colsample,\n",
    "                                    'min_child_weight': min_child,\n",
    "                                    'gamma': gamma,\n",
    "                                    'val_accuracy': float(val_acc),\n",
    "                                    'val_f1': float(f1)\n",
    "                                })\n",
    "                                \n",
    "                                if val_acc > best_stage1_acc:\n",
    "                                    best_stage1_acc = val_acc\n",
    "                                    best_stage1_params = params.copy()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STAGE 1 BEST: Acc={best_stage1_acc:.4f}\")\n",
    "    print(f\"  max_depth: {best_stage1_params['max_depth']}\")\n",
    "    print(f\"  learning_rate: {best_stage1_params['learning_rate']}\")\n",
    "    print(f\"  n_estimators: {best_stage1_params['n_estimators']}\")\n",
    "    print(f\"  min_child_weight: {best_stage1_params['min_child_weight']}\")\n",
    "    print(f\"  gamma: {best_stage1_params['gamma']}\")\n",
    "    \n",
    "    # STAGE 2: Fine-tune around best parameters - 81 combinations (~15-20 min)\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"STAGE 2: FINE-TUNING\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Create refined grid around best parameters\n",
    "    best_depth = best_stage1_params['max_depth']\n",
    "    best_lr = best_stage1_params['learning_rate']\n",
    "    best_n = best_stage1_params['n_estimators']\n",
    "    \n",
    "    stage2_grid = {\n",
    "        'max_depth': [max(3, best_depth-1), best_depth, min(10, best_depth+1)],\n",
    "        'learning_rate': [max(0.01, best_lr*0.8), best_lr, min(0.5, best_lr*1.2)],\n",
    "        'n_estimators': [max(50, best_n-50), best_n, best_n+50],\n",
    "        'subsample': [0.75, 0.8, 0.85],         # 3 options now\n",
    "        'colsample_bytree': [0.75, 0.8, 0.85],  # 3 options now\n",
    "        'min_child_weight': [best_stage1_params['min_child_weight']],\n",
    "        'gamma': [best_stage1_params['gamma']],\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    best_final_acc = 0\n",
    "    best_final_params = None\n",
    "    stage2_history = []\n",
    "    \n",
    "    total_stage2 = (len(stage2_grid['max_depth']) * len(stage2_grid['learning_rate']) * \n",
    "                    len(stage2_grid['n_estimators']) * len(stage2_grid['subsample']) * \n",
    "                    len(stage2_grid['colsample_bytree']))\n",
    "    \n",
    "    print(f\"Testing {total_stage2} refined combinations...\\n\")\n",
    "    \n",
    "    current = 0\n",
    "    for max_depth in stage2_grid['max_depth']:\n",
    "        for lr in stage2_grid['learning_rate']:\n",
    "            for n_est in stage2_grid['n_estimators']:\n",
    "                for subsample in stage2_grid['subsample']:\n",
    "                    for colsample in stage2_grid['colsample_bytree']:\n",
    "                        current += 1\n",
    "                        \n",
    "                        params = {\n",
    "                            'objective': 'multi:softmax',\n",
    "                            'num_class': 3,\n",
    "                            'max_depth': max_depth,\n",
    "                            'learning_rate': lr,\n",
    "                            'n_estimators': n_est,\n",
    "                            'subsample': subsample,\n",
    "                            'colsample_bytree': colsample,\n",
    "                            'min_child_weight': stage2_grid['min_child_weight'][0],\n",
    "                            'gamma': stage2_grid['gamma'][0],\n",
    "                            'random_state': 42,\n",
    "                            'eval_metric': 'mlogloss',\n",
    "                            'early_stopping_rounds': 20\n",
    "                        }\n",
    "                        \n",
    "                        model = xgb.XGBClassifier(**params)\n",
    "                        model.fit(X_train, y_train,\n",
    "                                 eval_set=[(X_val, y_val)],\n",
    "                                 verbose=False)\n",
    "                        \n",
    "                        y_val_pred = model.predict(X_val)\n",
    "                        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "                        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                            y_val, y_val_pred, average='macro', zero_division=0\n",
    "                        )\n",
    "                        \n",
    "                        if current % 10 == 0 or val_acc > best_final_acc:\n",
    "                            print(f\"[{current:2d}/{total_stage2}] d={max_depth}, lr={lr:.3f}, n={n_est:3d}, \"\n",
    "                                  f\"ss={subsample:.2f}, cs={colsample:.2f} | Acc={val_acc:.4f}, F1={f1:.4f}\")\n",
    "                        \n",
    "                        stage2_history.append({\n",
    "                            'max_depth': max_depth,\n",
    "                            'learning_rate': lr,\n",
    "                            'n_estimators': n_est,\n",
    "                            'subsample': subsample,\n",
    "                            'colsample_bytree': colsample,\n",
    "                            'val_accuracy': float(val_acc),\n",
    "                            'val_f1': float(f1)\n",
    "                        })\n",
    "                        \n",
    "                        if val_acc > best_final_acc:\n",
    "                            best_final_acc = val_acc\n",
    "                            best_final_params = params\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL BEST PARAMETERS (Val Accuracy: {best_final_acc:.4f})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  max_depth: {best_final_params['max_depth']}\")\n",
    "    print(f\"  learning_rate: {best_final_params['learning_rate']:.4f}\")\n",
    "    print(f\"  n_estimators: {best_final_params['n_estimators']}\")\n",
    "    print(f\"  subsample: {best_final_params['subsample']:.2f}\")\n",
    "    print(f\"  colsample_bytree: {best_final_params['colsample_bytree']:.2f}\")\n",
    "    print(f\"  min_child_weight: {best_final_params['min_child_weight']}\")\n",
    "    print(f\"  gamma: {best_final_params['gamma']}\")\n",
    "    \n",
    "    full_history = {\n",
    "        'stage1': stage1_history,\n",
    "        'stage2': stage2_history,\n",
    "        'stage1_best': best_stage1_params,\n",
    "        'stage2_best': best_final_params\n",
    "    }\n",
    "    \n",
    "    return best_final_params, full_history\n",
    "\n",
    "#####################################\n",
    "# 7. Train final model\n",
    "#####################################\n",
    "def train_final_model(X_train, y_train, X_val, y_val, X_test, y_test, params):\n",
    "    \"\"\"Train final XGBoost model with best parameters.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 6: TRAINING FINAL MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train,\n",
    "             eval_set=[(X_val, y_val)],\n",
    "             verbose=False)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    y_train_proba = model.predict_proba(X_train)\n",
    "    y_val_proba = model.predict_proba(X_val)\n",
    "    y_test_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, y_true, y_pred in [\n",
    "        ('train', y_train, y_train_pred),\n",
    "        ('val', y_val, y_val_pred),\n",
    "        ('test', y_test, y_test_pred)\n",
    "    ]:\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average='macro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': float(acc),\n",
    "            'precision': float(precision),\n",
    "            'recall': float(recall),\n",
    "            'f1': float(f1)\n",
    "        }\n",
    "    \n",
    "    print(\"\\nMODEL PERFORMANCE:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'Set':<10} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    for name in ['train', 'val', 'test']:\n",
    "        r = results[name]\n",
    "        print(f\"{name.capitalize():<10} {r['accuracy']:.4f}       {r['precision']:.4f}       \"\n",
    "              f\"{r['recall']:.4f}       {r['f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DETAILED TEST SET CLASSIFICATION REPORT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(classification_report(y_test, y_test_pred, \n",
    "                                target_names=['Down (0)', 'Neutral (1)', 'Up (2)']))\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix (Test Set):\")\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    return model, results, y_train_proba, y_val_proba, y_test_proba\n",
    "\n",
    "#####################################\n",
    "# 8. Save outputs\n",
    "#####################################\n",
    "def save_outputs(model, results, tuning_history, selected_features, scaler,\n",
    "                 X_train, X_val, X_test,\n",
    "                 y_train, y_val, y_test,\n",
    "                 y_train_proba, y_val_proba, y_test_proba):\n",
    "    \"\"\"Save all outputs.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 7: SAVING OUTPUTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Saving to: {OUTPUT_DIR}\")\n",
    "    \n",
    "    model.save_model(str(OUTPUT_DIR / 'xgb_best_model.json'))\n",
    "    print(\"✓ xgb_best_model.json\")\n",
    "    \n",
    "    history = {\n",
    "        'hyperparameter_tuning': tuning_history,\n",
    "        'final_results': results,\n",
    "        'num_features': len(selected_features),\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(OUTPUT_DIR / 'xgb_training_history.json', 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(\"✓ xgb_training_history.json\")\n",
    "    \n",
    "    joblib.dump(selected_features, OUTPUT_DIR / 'selected_features.pkl')\n",
    "    print(\"✓ selected_features.pkl\")\n",
    "    \n",
    "    joblib.dump(scaler, OUTPUT_DIR / 'scaler.pkl')\n",
    "    print(\"✓ scaler.pkl\")\n",
    "    \n",
    "    # Save predictions\n",
    "    train_predictions = pd.DataFrame({\n",
    "        'actual': y_train.values,\n",
    "        'predicted': y_train_proba.argmax(axis=1),\n",
    "        'prob_down': y_train_proba[:, 0],\n",
    "        'prob_neutral': y_train_proba[:, 1],\n",
    "        'prob_up': y_train_proba[:, 2]\n",
    "    })\n",
    "    train_predictions.to_csv(OUTPUT_DIR / 'xgb_train_predictions.csv', index=False)\n",
    "    print(\"✓ xgb_train_predictions.csv\")\n",
    "    \n",
    "    val_predictions = pd.DataFrame({\n",
    "        'actual': y_val.values,\n",
    "        'predicted': y_val_proba.argmax(axis=1),\n",
    "        'prob_down': y_val_proba[:, 0],\n",
    "        'prob_neutral': y_val_proba[:, 1],\n",
    "        'prob_up': y_val_proba[:, 2]\n",
    "    })\n",
    "    val_predictions.to_csv(OUTPUT_DIR / 'xgb_val_predictions.csv', index=False)\n",
    "    print(\"✓ xgb_val_predictions.csv\")\n",
    "    \n",
    "    test_predictions = pd.DataFrame({\n",
    "        'actual': y_test.values,\n",
    "        'predicted': y_test_proba.argmax(axis=1),\n",
    "        'prob_down': y_test_proba[:, 0],\n",
    "        'prob_neutral': y_test_proba[:, 1],\n",
    "        'prob_up': y_test_proba[:, 2]\n",
    "    })\n",
    "    test_predictions.to_csv(OUTPUT_DIR / 'xgb_test_predictions.csv', index=False)\n",
    "    print(\"✓ xgb_test_predictions.csv (Day 5 - ready for agent!)\")\n",
    "\n",
    "#####################################\n",
    "# 9. Main execution\n",
    "#####################################\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"XGBOOST BALANCED PIPELINE\")\n",
    "    print(\"Fast Two-Stage Tuning (~30-45 min total)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    all_dates = ['20251020', '20251021', '20251022', '20251023', '20251024']\n",
    "    df = fetch_orderbook_data(dates=all_dates)\n",
    "    \n",
    "    # Step 2: Create features\n",
    "    features = add_all_features(df)\n",
    "    features = add_labels(features, horizon=23)\n",
    "    \n",
    "    # Step 3: Feature selection (using training data only)\n",
    "    train_dates = ['20251020', '20251021', '20251022']\n",
    "    val_dates = ['20251023']\n",
    "    test_dates = ['20251024']\n",
    "    \n",
    "    train_features = features[features['date'].isin(train_dates)].copy()\n",
    "    selected_features = select_features(train_features, number_events_ahead=10, max_corr=0.85)\n",
    "    \n",
    "    # Step 4: Normalize and split\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, scaler = normalize_and_split(\n",
    "        features, selected_features, train_dates, val_dates, test_dates\n",
    "    )\n",
    "    \n",
    "    # Step 5: Two-stage hyperparameter tuning (BALANCED VERSION)\n",
    "    best_params, tuning_history = tune_hyperparameters_two_stage(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Step 6: Train final model\n",
    "    model, results, y_train_proba, y_val_proba, y_test_proba = train_final_model(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # Step 7: Save everything\n",
    "    save_outputs(model, results, tuning_history, selected_features, scaler,\n",
    "                 X_train, X_val, X_test,\n",
    "                 y_train, y_val, y_test,\n",
    "                 y_train_proba, y_val_proba, y_test_proba)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nModel Performance Summary:\")\n",
    "    print(f\"  Test Accuracy: {results['test']['accuracy']:.4f}\")\n",
    "    print(f\"  Test F1 Score: {results['test']['f1']:.4f}\")\n",
    "    print(f\"  Features Used: {len(selected_features)}\")\n",
    "    print(f\"\\nAll files saved to: {OUTPUT_DIR}\")\n",
    "    print(f\"\\nKey output: xgb_test_predictions.csv (ready for agent!)\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOP 10 MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    feature_importance = model.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    for idx, row in importance_df.head(10).iterrows():\n",
    "        print(f\"{row['feature']:30s} {row['importance']:.6f}\")\n",
    "    \n",
    "    # Save feature importance\n",
    "    importance_df.to_csv(OUTPUT_DIR / 'feature_importance.csv', index=False)\n",
    "    print(\"\\n✓ feature_importance.csv saved\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TUNING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Stage 1: Tested 32 combinations (coarse search)\")\n",
    "    print(f\"Stage 2: Tested 81 combinations (fine-tuning)\")\n",
    "    print(f\"Total combinations evaluated: 113\")\n",
    "    print(f\"\\nEstimated runtime: 30-45 minutes\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
